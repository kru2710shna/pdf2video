{"chunk_id": "YOUR_0000", "text": "[PAGE 1]\nDenoising Diffusion Probabilistic Models\nJonathan Ho\nUC Berkeley\njonathanho@berkeley.edu\nAjay Jain\nUC Berkeley\najayj@berkeley.edu\nPieter Abbeel\nUC Berkeley\npabbeel@cs.berkeley.edu\nAbstract\nWe present high quality image synthesis results using diffusion probabilistic models,\na class of latent variable models inspired by considerations from nonequilibrium\nthermodynamics. Our best results are obtained by training on a weighted variational\nbound designed according to a novel connection between diffusion probabilistic\nmodels and denoising score matching with Langevin dynamics, and our models nat-\nurally admit a progressive lossy decompression scheme that can be interpreted as a\ngeneralization of autoregressive decoding. On the unconditional CIFAR10 dataset,\nwe obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On\n256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our imple-\nmentation is available at https://github.com/hojonathanho/diffusion.\n1 Introduction\nDeep generative models of all kinds have recently exhibited high quality samples in a wide variety\nof data modalities. Generative adversarial networks (GANs), autoregressive models, ﬂows, and\nvariational autoencoders (V AEs) have synthesized striking image and audio samples [ 14, 27, 3,\n58, 38, 25, 10, 32, 44, 57, 26, 33, 45], and there have been remarkable advances in energy-based\nmodeling and score matching that have produced images comparable to those of GANs [11, 55].\nFigure 1: Generated samples on CelebA-HQ 256 ×256 (left) and unconditional CIFAR10 (right)\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:2006.11239v2  [cs.LG]  16 Dec 2020", "page_start": 1, "page_end": 1, "char_count": 1715, "text_raw": "[PAGE 1]\nDenoising Diffusion Probabilistic Models\nJonathan Ho\nUC Berkeley\njonathanho@berkeley.edu\nAjay Jain\nUC Berkeley\najayj@berkeley.edu\nPieter Abbeel\nUC Berkeley\npabbeel@cs.berkeley.edu\nAbstract\nWe present high quality image synthesis results using diffusion probabilistic models,\na class of latent variable models inspired by considerations from nonequilibrium\nthermodynamics. Our best results are obtained by training on a weighted variational\nbound designed according to a novel connection between diffusion probabilistic\nmodels and denoising score matching with Langevin dynamics, and our models nat-\nurally admit a progressive lossy decompression scheme that can be interpreted as a\ngeneralization of autoregressive decoding. On the unconditional CIFAR10 dataset,\nwe obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On\n256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our imple-\nmentation is available at https://github.com/hojonathanho/diffusion.\n1 Introduction\nDeep generative models of all kinds have recently exhibited high quality samples in a wide variety\nof data modalities. Generative adversarial networks (GANs), autoregressive models, ﬂows, and\nvariational autoencoders (V AEs) have synthesized striking image and audio samples [ 14, 27, 3,\n58, 38, 25, 10, 32, 44, 57, 26, 33, 45], and there have been remarkable advances in energy-based\nmodeling and score matching that have produced images comparable to those of GANs [11, 55].\nFigure 1: Generated samples on CelebA-HQ 256 ×256 (left) and unconditional CIFAR10 (right)\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:2006.11239v2  [cs.LG]  16 Dec 2020", "section": "abstract", "text_embed": "Abstract\nWe present high quality image synthesis results using diffusion probabilistic models,\na class of latent variable models inspired by considerations from nonequilibrium\nthermodynamics. Our best results are obtained by training on a weighted variational\nbound designed according to a novel connection between diffusion probabilistic\nmodels and denoising score matching with Langevin dynamics, and our models nat-\nurally admit a progressive lossy decompression scheme that can be interpreted as a\ngeneralization of autoregressive decoding. On the unconditional CIFAR10 dataset,\nwe obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On\n256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our imple-\nmentation is available at https://github.com/hojonathanho/diffusion.\n1 Introduction\nDeep generative models of all kinds have recently exhibited high quality samples in a wide variety\nof data modalities. Generative adversarial networks (GANs), autoregressive models, ﬂows, and\nvariational autoencoders (V AEs) have synthesized striking image and audio samples [ 14, 27, 3,\n58, 38, 25, 10, 32, 44, 57, 26, 33, 45], and there have been remarkable advances in energy-based\nmodeling and score matching that have produced images comparable to those of GANs [11, 55].\nFigure 1: Generated samples on CelebA-HQ 256 ×256 (left) and unconditional CIFAR10 (right)\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:2006.11239v2  [cs.LG]  16 Dec 2020"}
{"chunk_id": "YOUR_0001", "text": "to those of GANs [11, 55].\nFigure 1: Generated samples on CelebA-HQ 256 ×256 (left) and unconditional CIFAR10 (right)\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:2006.11239v2  [cs.LG]  16 Dec 2020\n\n[PAGE 2]\n\u0000!\n<latexit sha1_base64=\"7yFrn0YPyuP5dVIvc7Tl2zcbS/g=\">AAAB+HicbVBNSwMxEJ2tX7V+dNWjl2ARPJXdKuix6MVjBfsB7VKyaXYbmk2WJKvU0l/ixYMiXv0p3vw3pu0etPXBwOO9GWbmhSln2njet1NYW9/Y3Cpul3Z29/bL7sFhS8tMEdokkkvVCbGmnAnaNMxw2kkVxUnIaTsc3cz89gNVmklxb8YpDRIcCxYxgo2V+m65x6WIFYuHBislH/tuxat6c6BV4uekAjkafferN5AkS6gwhGOtu76XmmCClWGE02mpl2maYjLCMe1aKnBCdTCZHz5Fp1YZoEgqW8Kgufp7YoITrcdJaDsTbIZ62ZuJ/3ndzERXwYSJNDNUkMWiKOPISDRLAQ2YosTwsSWYKGZvRWSIFSbGZlWyIfjLL6+SVq3qn1drdxeV+nUeRxGO4QTOwIdLqMMtNKAJBDJ4hld4c56cF+fd+Vi0Fpx85gj+wPn8AXOGk5o=</latexit>\nx T \u0000! ··· \u0000! x t \u0000\u0000\u0000\u0000\u0000! x t \u0000 1 \u0000! ··· \u0000! x 0\n<latexit sha1_base64=\"l4LvSgM7PR7I/kkuy5soikK4gpU=\">AAAEoXictVLditNAFE7XqGv92a5eejOYLexKLU0VFKRQ9EYvhCrb3YUklOlk2g6dnzBzYrcb8zK+lU/gazhJK6atuiB4YODM+T/n+8YJZwY6nW+1vRvuzVu39+/U7967/+CgcfjwzKhUEzokiit9McaGcibpEBhwepFoisWY0/Px/G3hP/9MtWFKnsIyoZHAU8kmjGCwplHjeygwzAjThNM4Kz/jSXaZj05zFHIlp5pNZ4C1VgsUkliB2TX/oQLYCpe/4rJwZhJM6NPMJyLPt9IM0SwBA0tOUaVGBs/8/J8mWVRH6eSjhtdpd0pBu4q/VjxnLYPR4d7XMFYkFVQC4diYwO8kEGVYA7P183qYGmr3meMpDawqsaAmykpEctS0lhhNlLZPAiqt1YwMC2OWYmwjiynNtq8w/s4XpDB5FWVMJilQSVaNJilHoFABL4qZpgT40irYntTOisgMa0zAkqC+0QbY/MquIfCcYssbsBH1UNIFUUJgGVePGfhR1qyj1YETXAaH/SqAnp836/lGftUfdNcFiqbBT8L2jouQdvE9iVAoVUyDWONFa5XVYlJSjezEPT+BlmCSiVQgw65or2vBaE0Y5z1e4D/VeBmhstwJyo5C0YeZ53vdo/z19lhVjly71+K6xRb/ZbO/rbLCS8HMwmVZ7W9zeFc567b95+3uxxde/82a3/vOY+eJc+z4zkun77xzBs7QIbUPNVP7Ustdz33vDtxPq9C92jrnkbMhbvAD81mObw==</latexit>\np ✓ ( x t \u0000 1 | x t )\n<latexit sha1_base64=\"XVzP503G8Ma8Lkwk3KKGZcZJbZ0=\">AAACEnicbVC7SgNBFJ2Nrxhfq5Y2g0FICsNuFEwZsLGMYB6QLMvsZDYZMvtg5q4Y1nyDjb9iY6GIrZWdf+Mk2SImHrhwOOde7r3HiwVXYFk/Rm5tfWNzK79d2Nnd2z8wD49aKkokZU0aiUh2PKKY4CFrAgfBOrFkJPAEa3uj66nfvmdS8Si8g3HMnIAMQu5zSkBLrlmO3R4MGZBSLyAw9Pz0YeKmcG5P8CNekKDsmkWrYs2AV4mdkSLK0HDN714/oknAQqCCKNW1rRiclEjgVLBJoZcoFhM6IgPW1TQkAVNOOntpgs+00sd+JHWFgGfq4kRKAqXGgac7p0eqZW8q/ud1E/BrTsrDOAEW0vkiPxEYIjzNB/e5ZBTEWBNCJde3YjokklDQKRZ0CPbyy6ukVa3YF5Xq7WWxXsviyKMTdIpKyEZXqI5uUAM1EUVP6AW9oXfj2Xg1PozPeWvOyGaO0R8YX7+bCp4F</latexit>\nq ( x t | x t \u0000 1 )\n<latexit sha1_base64=\"eAZ87UuTmAQoJ4u19RGH5tA+bCI=\">AAACC3icbVC7TgJBFJ31ifhatbSZQEywkOyiiZQkNpaYyCMBspkdZmHC7MOZu0ay0tv4KzYWGmPrD9j5N87CFgieZJIz59ybe+9xI8EVWNaPsbK6tr6xmdvKb+/s7u2bB4dNFcaSsgYNRSjbLlFM8IA1gINg7Ugy4ruCtdzRVeq37plUPAxuYRyxnk8GAfc4JaAlxyzclbo+ga", "page_start": 2, "page_end": 2, "char_count": 2500, "text_raw": "to those of GANs [11, 55].\nFigure 1: Generated samples on CelebA-HQ 256 ×256 (left) and unconditional CIFAR10 (right)\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:2006.11239v2  [cs.LG]  16 Dec 2020\n\n[PAGE 2]\n\u0000!\n<latexit sha1_base64=\"7yFrn0YPyuP5dVIvc7Tl2zcbS/g=\">AAAB+HicbVBNSwMxEJ2tX7V+dNWjl2ARPJXdKuix6MVjBfsB7VKyaXYbmk2WJKvU0l/ixYMiXv0p3vw3pu0etPXBwOO9GWbmhSln2njet1NYW9/Y3Cpul3Z29/bL7sFhS8tMEdokkkvVCbGmnAnaNMxw2kkVxUnIaTsc3cz89gNVmklxb8YpDRIcCxYxgo2V+m65x6WIFYuHBislH/tuxat6c6BV4uekAjkafferN5AkS6gwhGOtu76XmmCClWGE02mpl2maYjLCMe1aKnBCdTCZHz5Fp1YZoEgqW8Kgufp7YoITrcdJaDsTbIZ62ZuJ/3ndzERXwYSJNDNUkMWiKOPISDRLAQ2YosTwsSWYKGZvRWSIFSbGZlWyIfjLL6+SVq3qn1drdxeV+nUeRxGO4QTOwIdLqMMtNKAJBDJ4hld4c56cF+fd+Vi0Fpx85gj+wPn8AXOGk5o=</latexit>\nx T \u0000! ··· \u0000! x t \u0000\u0000\u0000\u0000\u0000! x t \u0000 1 \u0000! ··· \u0000! x 0\n<latexit sha1_base64=\"l4LvSgM7PR7I/kkuy5soikK4gpU=\">AAAEoXictVLditNAFE7XqGv92a5eejOYLexKLU0VFKRQ9EYvhCrb3YUklOlk2g6dnzBzYrcb8zK+lU/gazhJK6atuiB4YODM+T/n+8YJZwY6nW+1vRvuzVu39+/U7967/+CgcfjwzKhUEzokiit9McaGcibpEBhwepFoisWY0/Px/G3hP/9MtWFKnsIyoZHAU8kmjGCwplHjeygwzAjThNM4Kz/jSXaZj05zFHIlp5pNZ4C1VgsUkliB2TX/oQLYCpe/4rJwZhJM6NPMJyLPt9IM0SwBA0tOUaVGBs/8/J8mWVRH6eSjhtdpd0pBu4q/VjxnLYPR4d7XMFYkFVQC4diYwO8kEGVYA7P183qYGmr3meMpDawqsaAmykpEctS0lhhNlLZPAiqt1YwMC2OWYmwjiynNtq8w/s4XpDB5FWVMJilQSVaNJilHoFABL4qZpgT40irYntTOisgMa0zAkqC+0QbY/MquIfCcYssbsBH1UNIFUUJgGVePGfhR1qyj1YETXAaH/SqAnp836/lGftUfdNcFiqbBT8L2jouQdvE9iVAoVUyDWONFa5XVYlJSjezEPT+BlmCSiVQgw65or2vBaE0Y5z1e4D/VeBmhstwJyo5C0YeZ53vdo/z19lhVjly71+K6xRb/ZbO/rbLCS8HMwmVZ7W9zeFc567b95+3uxxde/82a3/vOY+eJc+z4zkun77xzBs7QIbUPNVP7Ustdz33vDtxPq9C92jrnkbMhbvAD81mObw==</latexit>\np ✓ ( x t \u0000 1 | x t )\n<latexit sha1_base64=\"XVzP503G8Ma8Lkwk3KKGZcZJbZ0=\">AAACEnicbVC7SgNBFJ2Nrxhfq5Y2g0FICsNuFEwZsLGMYB6QLMvsZDYZMvtg5q4Y1nyDjb9iY6GIrZWdf+Mk2SImHrhwOOde7r3HiwVXYFk/Rm5tfWNzK79d2Nnd2z8wD49aKkokZU0aiUh2PKKY4CFrAgfBOrFkJPAEa3uj66nfvmdS8Si8g3HMnIAMQu5zSkBLrlmO3R4MGZBSLyAw9Pz0YeKmcG5P8CNekKDsmkWrYs2AV4mdkSLK0HDN714/oknAQqCCKNW1rRiclEjgVLBJoZcoFhM6IgPW1TQkAVNOOntpgs+00sd+JHWFgGfq4kRKAqXGgac7p0eqZW8q/ud1E/BrTsrDOAEW0vkiPxEYIjzNB/e5ZBTEWBNCJde3YjokklDQKRZ0CPbyy6ukVa3YF5Xq7WWxXsviyKMTdIpKyEZXqI5uUAM1EUVP6AW9oXfj2Xg1PozPeWvOyGaO0R8YX7+bCp4F</latexit>\nq ( x t | x t \u0000 1 )\n<latexit sha1_base64=\"eAZ87UuTmAQoJ4u19RGH5tA+bCI=\">AAACC3icbVC7TgJBFJ31ifhatbSZQEywkOyiiZQkNpaYyCMBspkdZmHC7MOZu0ay0tv4KzYWGmPrD9j5N87CFgieZJIz59ybe+9xI8EVWNaPsbK6tr6xmdvKb+/s7u2bB4dNFcaSsgYNRSjbLlFM8IA1gINg7Ugy4ruCtdzRVeq37plUPAxuYRyxnk8GAfc4JaAlxyzclbo+ga", "section": "unknown", "text_embed": "to those of GANs [11, 55].\nFigure 1: Generated samples on CelebA-HQ 256 ×256 (left) and unconditional CIFAR10 (right)\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:2006.11239v2  [cs.LG]  16 Dec 2020\n\n[PAGE 2]\n\u0000!\n<latexit sha1_base64=\"7yFrn0YPyuP5dVIvc7Tl2zcbS/g=\">AAAB+HicbVBNSwMxEJ2tX7V+dNWjl2ARPJXdKuix6MVjBfsB7VKyaXYbmk2WJKvU0l/ixYMiXv0p3vw3pu0etPXBwOO9GWbmhSln2njet1NYW9/Y3Cpul3Z29/bL7sFhS8tMEdokkkvVCbGmnAnaNMxw2kkVxUnIaTsc3cz89gNVmklxb8YpDRIcCxYxgo2V+m65x6WIFYuHBislH/tuxat6c6BV4uekAjkafferN5AkS6gwhGOtu76XmmCClWGE02mpl2maYjLCMe1aKnBCdTCZHz5Fp1YZoEgqW8Kgufp7YoITrcdJaDsTbIZ62ZuJ/3ndzERXwYSJNDNUkMWiKOPISDRLAQ2YosTwsSWYKGZvRWSIFSbGZlWyIfjLL6+SVq3qn1drdxeV+nUeRxGO4QTOwIdLqMMtNKAJBDJ4hld4c56cF+fd+Vi0Fpx85gj+wPn8AXOGk5o=</latexit>\nx T \u0000! ··· \u0000! x t \u0000\u0000\u0000\u0000\u0000! x t \u0000 1 \u0000! ··· \u0000! x 0\n<latexit sha1_base64=\"l4LvSgM7PR7I/kkuy5soikK4gpU=\">AAAEoXictVLditNAFE7XqGv92a5eejOYLexKLU0VFKRQ9EYvhCrb3YUklOlk2g6dnzBzYrcb8zK+lU/gazhJK6atuiB4YODM+T/n+8YJZwY6nW+1vRvuzVu39+/U7967/+CgcfjwzKhUEzokiit9McaGcibpEBhwepFoisWY0/Px/G3hP/9MtWFKnsIyoZHAU8kmjGCwplHjeygwzAjThNM4Kz/jSXaZj05zFHIlp5pNZ4C1VgsUkliB2TX/oQLYCpe/4rJwZhJM6NPMJyLPt9IM0SwBA0tOUaVGBs/8/J8mWVRH6eSjhtdpd0pBu4q/VjxnLYPR4d7XMFYkFVQC4diYwO8kEGVYA7P183qYGmr3meMpDawqsaAmykpEctS0lhhNlLZPAiqt1YwMC2OWYmwjiynNtq8w/s4XpDB5FWVMJilQSVaNJilHoFABL4qZpgT40irYntTOisgMa0zAkqC+0QbY/MquIfCcYssbsBH1UNIFUUJgGVePGfhR1qyj1YETXAaH/SqAnp836/lGftUfdNcFiqbBT8L2jouQdvE9iVAoVUyDWONFa5XVYlJSjezEPT+BlmCSiVQgw65or2vBaE0Y5z1e4D/VeBmhstwJyo5C0YeZ53vdo/z19lhVjly71+K6xRb/ZbO/rbLCS8HMwmVZ7W9zeFc567b95+3uxxde/82a3/vOY+eJc+z4zkun77xzBs7QIbUPNVP7Ustdz33vDtxPq9C92jrnkbMhbvAD81mObw==</latexit>\np ✓ ( x t \u0000 1 | x t )\n<latexit sha1_base64=\"XVzP503G8Ma8Lkwk3KKGZcZJbZ0=\">AAACEnicbVC7SgNBFJ2Nrxhfq5Y2g0FICsNuFEwZsLGMYB6QLMvsZDYZMvtg5q4Y1nyDjb9iY6GIrZWdf+Mk2SImHrhwOOde7r3HiwVXYFk/Rm5tfWNzK79d2Nnd2z8wD49aKkokZU0aiUh2PKKY4CFrAgfBOrFkJPAEa3uj66nfvmdS8Si8g3HMnIAMQu5zSkBLrlmO3R4MGZBSLyAw9Pz0YeKmcG5P8CNekKDsmkWrYs2AV4mdkSLK0HDN714/oknAQqCCKNW1rRiclEjgVLBJoZcoFhM6IgPW1TQkAVNOOntpgs+00sd+JHWFgGfq4kRKAqXGgac7p0eqZW8q/ud1E/BrTsrDOAEW0vkiPxEYIjzNB/e5ZBTEWBNCJde3YjokklDQKRZ0CPbyy6ukVa3YF5Xq7WWxXsviyKMTdIpKyEZXqI5uUAM1EUVP6AW9oXfj2Xg1PozPeWvOyGaO0R8YX7+bCp4F</latexit>\nq ( x t | x t \u0000 1 )\n<latexit sha1_base64=\"eAZ87UuTmAQoJ4u19RGH5tA+bCI=\">AAACC3icbVC7TgJBFJ31ifhatbSZQEywkOyiiZQkNpaYyCMBspkdZmHC7MOZu0ay0tv4KzYWGmPrD9j5N87CFgieZJIz59ybe+9xI8EVWNaPsbK6tr6xmdvKb+/s7u2bB4dNFcaSsgYNRSjbLlFM8IA1gINg7Ugy4ruCtdzRVeq37plUPAxuYRyxnk8GAfc4JaAlxyzclbo+ga"}
{"chunk_id": "YOUR_0002", "text": "sha1_base64=\"eAZ87UuTmAQoJ4u19RGH5tA+bCI=\">AAACC3icbVC7TgJBFJ31ifhatbSZQEywkOyiiZQkNpaYyCMBspkdZmHC7MOZu0ay0tv4KzYWGmPrD9j5N87CFgieZJIz59ybe+9xI8EVWNaPsbK6tr6xmdvKb+/s7u2bB4dNFcaSsgYNRSjbLlFM8IA1gINg7Ugy4ruCtdzRVeq37plUPAxuYRyxnk8GAfc4JaAlxyzclbo+gaHrJQ8TB/AjnvsmcGZPTh2zaJWtKfAysTNSRBnqjvnd7Yc09lkAVBClOrYVQS8hEjgVbJLvxopFhI7IgHU0DYjPVC+Z3jLBJ1rpYy+U+gWAp+p8R0J8pca+qyvTRdWil4r/eZ0YvGov4UEUAwvobJAXCwwhToPBfS4ZBTHWhFDJ9a6YDokkFHR8eR2CvXjyMmlWyvZ5uXJzUaxVszhy6BgVUAnZ6BLV0DWqowai6Am9oDf0bjwbr8aH8TkrXTGyniP0B8bXL+1hmu8=</latexit>\nFigure 2: The directed graphical model considered in this work.\nThis paper presents progress in diffusion probabilistic models [53]. A diffusion probabilistic model\n(which we will call a “diffusion model” for brevity) is a parameterized Markov chain trained using\nvariational inference to produce samples matching the data after ﬁnite time. Transitions of this chain\nare learned to reverse a diffusion process, which is a Markov chain that gradually adds noise to the\ndata in the opposite direction of sampling until signal is destroyed. When the diffusion consists of\nsmall amounts of Gaussian noise, it is sufﬁcient to set the sampling chain transitions to conditional\nGaussians too, allowing for a particularly simple neural network parameterization.\nDiffusion models are straightforward to deﬁne and efﬁcient to train, but to the best of our knowledge,\nthere has been no demonstration that they are capable of generating high quality samples. We\nshow that diffusion models actually are capable of generating high quality samples, sometimes\nbetter than the published results on other types of generative models (Section 4). In addition, we\nshow that a certain parameterization of diffusion models reveals an equivalence with denoising\nscore matching over multiple noise levels during training and with annealed Langevin dynamics\nduring sampling (Section 3.2) [ 55, 61]. We obtained our best sample quality results using this\nparameterization (Section 4.2), so we consider this equivalence to be one of our primary contributions.\nDespite their sample quality, our models do not have competitive log likelihoods compared to other\nlikelihood-based models (our models do, however, have log likelihoods better than the large estimates\nannealed importance sampling has been reported to produce for energy based models and score\nmatching [11, 55]). We ﬁnd that the majority of our models’ lossless codelengths are consumed\nto describe imperceptible image details (Section 4.", "page_start": 1, "page_end": 1, "char_count": 2500, "text_raw": "sha1_base64=\"eAZ87UuTmAQoJ4u19RGH5tA+bCI=\">AAACC3icbVC7TgJBFJ31ifhatbSZQEywkOyiiZQkNpaYyCMBspkdZmHC7MOZu0ay0tv4KzYWGmPrD9j5N87CFgieZJIz59ybe+9xI8EVWNaPsbK6tr6xmdvKb+/s7u2bB4dNFcaSsgYNRSjbLlFM8IA1gINg7Ugy4ruCtdzRVeq37plUPAxuYRyxnk8GAfc4JaAlxyzclbo+gaHrJQ8TB/AjnvsmcGZPTh2zaJWtKfAysTNSRBnqjvnd7Yc09lkAVBClOrYVQS8hEjgVbJLvxopFhI7IgHU0DYjPVC+Z3jLBJ1rpYy+U+gWAp+p8R0J8pca+qyvTRdWil4r/eZ0YvGov4UEUAwvobJAXCwwhToPBfS4ZBTHWhFDJ9a6YDokkFHR8eR2CvXjyMmlWyvZ5uXJzUaxVszhy6BgVUAnZ6BLV0DWqowai6Am9oDf0bjwbr8aH8TkrXTGyniP0B8bXL+1hmu8=</latexit>\nFigure 2: The directed graphical model considered in this work.\nThis paper presents progress in diffusion probabilistic models [53]. A diffusion probabilistic model\n(which we will call a “diffusion model” for brevity) is a parameterized Markov chain trained using\nvariational inference to produce samples matching the data after ﬁnite time. Transitions of this chain\nare learned to reverse a diffusion process, which is a Markov chain that gradually adds noise to the\ndata in the opposite direction of sampling until signal is destroyed. When the diffusion consists of\nsmall amounts of Gaussian noise, it is sufﬁcient to set the sampling chain transitions to conditional\nGaussians too, allowing for a particularly simple neural network parameterization.\nDiffusion models are straightforward to deﬁne and efﬁcient to train, but to the best of our knowledge,\nthere has been no demonstration that they are capable of generating high quality samples. We\nshow that diffusion models actually are capable of generating high quality samples, sometimes\nbetter than the published results on other types of generative models (Section 4). In addition, we\nshow that a certain parameterization of diffusion models reveals an equivalence with denoising\nscore matching over multiple noise levels during training and with annealed Langevin dynamics\nduring sampling (Section 3.2) [ 55, 61]. We obtained our best sample quality results using this\nparameterization (Section 4.2), so we consider this equivalence to be one of our primary contributions.\nDespite their sample quality, our models do not have competitive log likelihoods compared to other\nlikelihood-based models (our models do, however, have log likelihoods better than the large estimates\nannealed importance sampling has been reported to produce for energy based models and score\nmatching [11, 55]). We ﬁnd that the majority of our models’ lossless codelengths are consumed\nto describe imperceptible image details (Section 4.", "section": "unknown", "text_embed": "sha1_base64=\"eAZ87UuTmAQoJ4u19RGH5tA+bCI=\">AAACC3icbVC7TgJBFJ31ifhatbSZQEywkOyiiZQkNpaYyCMBspkdZmHC7MOZu0ay0tv4KzYWGmPrD9j5N87CFgieZJIz59ybe+9xI8EVWNaPsbK6tr6xmdvKb+/s7u2bB4dNFcaSsgYNRSjbLlFM8IA1gINg7Ugy4ruCtdzRVeq37plUPAxuYRyxnk8GAfc4JaAlxyzclbo+gaHrJQ8TB/AjnvsmcGZPTh2zaJWtKfAysTNSRBnqjvnd7Yc09lkAVBClOrYVQS8hEjgVbJLvxopFhI7IgHU0DYjPVC+Z3jLBJ1rpYy+U+gWAp+p8R0J8pca+qyvTRdWil4r/eZ0YvGov4UEUAwvobJAXCwwhToPBfS4ZBTHWhFDJ9a6YDokkFHR8eR2CvXjyMmlWyvZ5uXJzUaxVszhy6BgVUAnZ6BLV0DWqowai6Am9oDf0bjwbr8aH8TkrXTGyniP0B8bXL+1hmu8=</latexit>\nFigure 2: The directed graphical model considered in this work.\nThis paper presents progress in diffusion probabilistic models [53]. A diffusion probabilistic model\n(which we will call a “diffusion model” for brevity) is a parameterized Markov chain trained using\nvariational inference to produce samples matching the data after ﬁnite time. Transitions of this chain\nare learned to reverse a diffusion process, which is a Markov chain that gradually adds noise to the\ndata in the opposite direction of sampling until signal is destroyed. When the diffusion consists of\nsmall amounts of Gaussian noise, it is sufﬁcient to set the sampling chain transitions to conditional\nGaussians too, allowing for a particularly simple neural network parameterization.\nDiffusion models are straightforward to deﬁne and efﬁcient to train, but to the best of our knowledge,\nthere has been no demonstration that they are capable of generating high quality samples. We\nshow that diffusion models actually are capable of generating high quality samples, sometimes\nbetter than the published results on other types of generative models (Section 4). In addition, we\nshow that a certain parameterization of diffusion models reveals an equivalence with denoising\nscore matching over multiple noise levels during training and with annealed Langevin dynamics\nduring sampling (Section 3.2) [ 55, 61]. We obtained our best sample quality results using this\nparameterization (Section 4.2), so we consider this equivalence to be one of our primary contributions.\nDespite their sample quality, our models do not have competitive log likelihoods compared to other\nlikelihood-based models (our models do, however, have log likelihoods better than the large estimates\nannealed importance sampling has been reported to produce for energy based models and score\nmatching [11, 55]). We ﬁnd that the majority of our models’ lossless codelengths are consumed\nto describe imperceptible image details (Section 4."}
{"chunk_id": "YOUR_0003", "text": "ge estimates\nannealed importance sampling has been reported to produce for energy based models and score\nmatching [11, 55]). We ﬁnd that the majority of our models’ lossless codelengths are consumed\nto describe imperceptible image details (Section 4.3). We present a more reﬁned analysis of this\nphenomenon in the language of lossy compression, and we show that the sampling procedure of\ndiffusion models is a type of progressive decoding that resembles autoregressive decoding along a bit\nordering that vastly generalizes what is normally possible with autoregressive models.\n2 Background\nDiffusion models [53] are latent variable models of the form pθ(x0) :=\n∫\npθ(x0:T) dx1:T, where\nx1,..., xT are latents of the same dimensionality as the data x0 ∼q(x0). The joint distribution\npθ(x0:T) is called the reverse process, and it is deﬁned as a Markov chain with learned Gaussian\ntransitions starting at p(xT) = N(xT; 0,I):\npθ(x0:T) := p(xT)\nT∏\nt=1\npθ(xt−1|xt), p θ(xt−1|xt) := N(xt−1; µθ(xt,t),Σθ(xt,t)) (1)\nWhat distinguishes diffusion models from other types of latent variable models is that the approximate\nposterior q(x1:T|x0), called the forward processor diffusion process, is ﬁxed to a Markov chain that\ngradually adds Gaussian noise to the data according to a variance schedule β1,...,β T:\nq(x1:T|x0) :=\nT∏\nt=1\nq(xt|xt−1), q (xt|xt−1) := N(xt;\n√\n1 −βtxt−1,βtI) (2)\nTraining is performed by optimizing the usual variational bound on negative log likelihood:\nE[−log pθ(x0)] ≤Eq\n[\n−log pθ(x0:T)\nq(x1:T|x0)\n]\n= Eq\n[\n−log p(xT) −\n∑\nt≥1\nlog pθ(xt−1|xt)\nq(xt|xt−1)\n]\n=: L (3)\nThe forward process variances βt can be learned by reparameterization [ 33] or held constant as\nhyperparameters, and expressiveness of the reverse process is ensured in part by the choice of\nGaussian conditionals in pθ(xt−1|xt), because both processes have the same functional form when\nβt are small [ 53]. A notable property of the forward process is that it admits sampling xt at an\narbitrary timestep tin closed form: using the notation αt := 1 −βt and ¯αt := ∏t\ns=1 αs, we have\nq(xt|x0) = N(xt; √¯αtx0,(1 −¯αt)I) (4)\n2", "page_start": 1, "page_end": 1, "char_count": 2099, "text_raw": "ge estimates\nannealed importance sampling has been reported to produce for energy based models and score\nmatching [11, 55]). We ﬁnd that the majority of our models’ lossless codelengths are consumed\nto describe imperceptible image details (Section 4.3). We present a more reﬁned analysis of this\nphenomenon in the language of lossy compression, and we show that the sampling procedure of\ndiffusion models is a type of progressive decoding that resembles autoregressive decoding along a bit\nordering that vastly generalizes what is normally possible with autoregressive models.\n2 Background\nDiffusion models [53] are latent variable models of the form pθ(x0) :=\n∫\npθ(x0:T) dx1:T, where\nx1,..., xT are latents of the same dimensionality as the data x0 ∼q(x0). The joint distribution\npθ(x0:T) is called the reverse process, and it is deﬁned as a Markov chain with learned Gaussian\ntransitions starting at p(xT) = N(xT; 0,I):\npθ(x0:T) := p(xT)\nT∏\nt=1\npθ(xt−1|xt), p θ(xt−1|xt) := N(xt−1; µθ(xt,t),Σθ(xt,t)) (1)\nWhat distinguishes diffusion models from other types of latent variable models is that the approximate\nposterior q(x1:T|x0), called the forward processor diffusion process, is ﬁxed to a Markov chain that\ngradually adds Gaussian noise to the data according to a variance schedule β1,...,β T:\nq(x1:T|x0) :=\nT∏\nt=1\nq(xt|xt−1), q (xt|xt−1) := N(xt;\n√\n1 −βtxt−1,βtI) (2)\nTraining is performed by optimizing the usual variational bound on negative log likelihood:\nE[−log pθ(x0)] ≤Eq\n[\n−log pθ(x0:T)\nq(x1:T|x0)\n]\n= Eq\n[\n−log p(xT) −\n∑\nt≥1\nlog pθ(xt−1|xt)\nq(xt|xt−1)\n]\n=: L (3)\nThe forward process variances βt can be learned by reparameterization [ 33] or held constant as\nhyperparameters, and expressiveness of the reverse process is ensured in part by the choice of\nGaussian conditionals in pθ(xt−1|xt), because both processes have the same functional form when\nβt are small [ 53]. A notable property of the forward process is that it admits sampling xt at an\narbitrary timestep tin closed form: using the notation αt := 1 −βt and ¯αt := ∏t\ns=1 αs, we have\nq(xt|x0) = N(xt; √¯αtx0,(1 −¯αt)I) (4)\n2", "section": "unknown", "text_embed": "ge estimates\nannealed importance sampling has been reported to produce for energy based models and score\nmatching [11, 55]). We ﬁnd that the majority of our models’ lossless codelengths are consumed\nto describe imperceptible image details (Section 4.3). We present a more reﬁned analysis of this\nphenomenon in the language of lossy compression, and we show that the sampling procedure of\ndiffusion models is a type of progressive decoding that resembles autoregressive decoding along a bit\nordering that vastly generalizes what is normally possible with autoregressive models.\n2 Background\nDiffusion models [53] are latent variable models of the form pθ(x0) :=\n∫\npθ(x0:T) dx1:T, where\nx1,..., xT are latents of the same dimensionality as the data x0 ∼q(x0). The joint distribution\npθ(x0:T) is called the reverse process, and it is deﬁned as a Markov chain with learned Gaussian\ntransitions starting at p(xT) = N(xT; 0,I):\npθ(x0:T) := p(xT)\nT∏\nt=1\npθ(xt−1|xt), p θ(xt−1|xt) := N(xt−1; µθ(xt,t),Σθ(xt,t)) (1)\nWhat distinguishes diffusion models from other types of latent variable models is that the approximate\nposterior q(x1:T|x0), called the forward processor diffusion process, is ﬁxed to a Markov chain that\ngradually adds Gaussian noise to the data according to a variance schedule β1,...,β T:\nq(x1:T|x0) :=\nT∏\nt=1\nq(xt|xt−1), q (xt|xt−1) := N(xt;\n√\n1 −βtxt−1,βtI) (2)\nTraining is performed by optimizing the usual variational bound on negative log likelihood:\nE[−log pθ(x0)] ≤Eq\n[\n−log pθ(x0:T)\nq(x1:T|x0)\n]\n= Eq\n[\n−log p(xT) −\n∑\nt≥1\nlog pθ(xt−1|xt)\nq(xt|xt−1)\n]\n=: L (3)\nThe forward process variances βt can be learned by reparameterization [ 33] or held constant as\nhyperparameters, and expressiveness of the reverse process is ensured in part by the choice of\nGaussian conditionals in pθ(xt−1|xt), because both processes have the same functional form when\nβt are small [ 53]. A notable property of the forward process is that it admits sampling xt at an\narbitrary timestep tin closed form: using the notation αt := 1 −βt and ¯αt := ∏t\ns=1 αs, we have\nq(xt|x0) = N(xt; √¯αtx0,(1 −¯αt)I) (4)\n2"}
{"chunk_id": "YOUR_0004", "text": "onal form when\nβt are small [ 53]. A notable property of the forward process is that it admits sampling xt at an\narbitrary timestep tin closed form: using the notation αt := 1 −βt and ¯αt := ∏t\ns=1 αs, we have\nq(xt|x0) = N(xt; √¯αtx0,(1 −¯αt)I) (4)\n2\n\n[PAGE 3]\nEfﬁcient training is therefore possible by optimizing random terms of Lwith stochastic gradient\ndescent. Further improvements come from variance reduction by rewriting L(3) as:\nEq\n[\nDKL(q(xT|x0) ∥p(xT))  \nLT\n+\n∑\nt>1\nDKL(q(xt−1|xt,x0) ∥pθ(xt−1|xt))  \nLt−1\n−log pθ(x0|x1)  \nL0\n]\n(5)\n(See Appendix A for details. The labels on the terms are used in Section 3.) Equation (5) uses KL\ndivergence to directly compare pθ(xt−1|xt) against forward process posteriors, which are tractable\nwhen conditioned on x0:\nq(xt−1|xt,x0) = N(xt−1; ˜µt(xt,x0),˜βtI), (6)\nwhere ˜µt(xt,x0) :=\n√¯αt−1βt\n1 −¯αt\nx0 +\n√αt(1 −¯αt−1)\n1 −¯αt\nxt and ˜βt := 1 −¯αt−1\n1 −¯αt\nβt (7)\nConsequently, all KL divergences in Eq. (5) are comparisons between Gaussians, so they can be\ncalculated in a Rao-Blackwellized fashion with closed form expressions instead of high variance\nMonte Carlo estimates.\n3 Diffusion models and denoising autoencoders\nDiffusion models might appear to be a restricted class of latent variable models, but they allow a\nlarge number of degrees of freedom in implementation. One must choose the variances βt of the\nforward process and the model architecture and Gaussian distribution parameterization of the reverse\nprocess. To guide our choices, we establish a new explicit connection between diffusion models\nand denoising score matching (Section 3.2) that leads to a simpliﬁed, weighted variational bound\nobjective for diffusion models (Section 3.4). Ultimately, our model design is justiﬁed by simplicity\nand empirical results (Section 4). Our discussion is categorized by the terms of Eq. (5).\n3.1 Forward process and LT\nWe ignore the fact that the forward process variances βt are learnable by reparameterization and\ninstead ﬁx them to constants (see Section 4 for details). Thus, in our implementation, the approximate\nposterior qhas no learnable parameters, so LT is a constant during training and can be ignored.\n3.2 Reverse process and L1:T−1\nNow we discuss our choices in pθ(xt−1|xt) = N(xt−1; µθ(xt,t),Σθ(xt,t)) for 1 <t ≤T. First,\nwe set Σθ(xt,t) = σ2\ntI to untrained time dependent constants. Experimentally, both σ2\nt = βt and\nσ2\nt = ˜βt = 1−¯αt−1\n1−¯αt\nβt had similar results. The ﬁrst choice is optimal for x0 ∼N (0,I), and th", "page_start": 3, "page_end": 3, "char_count": 2500, "text_raw": "onal form when\nβt are small [ 53]. A notable property of the forward process is that it admits sampling xt at an\narbitrary timestep tin closed form: using the notation αt := 1 −βt and ¯αt := ∏t\ns=1 αs, we have\nq(xt|x0) = N(xt; √¯αtx0,(1 −¯αt)I) (4)\n2\n\n[PAGE 3]\nEfﬁcient training is therefore possible by optimizing random terms of Lwith stochastic gradient\ndescent. Further improvements come from variance reduction by rewriting L(3) as:\nEq\n[\nDKL(q(xT|x0) ∥p(xT))  \nLT\n+\n∑\nt>1\nDKL(q(xt−1|xt,x0) ∥pθ(xt−1|xt))  \nLt−1\n−log pθ(x0|x1)  \nL0\n]\n(5)\n(See Appendix A for details. The labels on the terms are used in Section 3.) Equation (5) uses KL\ndivergence to directly compare pθ(xt−1|xt) against forward process posteriors, which are tractable\nwhen conditioned on x0:\nq(xt−1|xt,x0) = N(xt−1; ˜µt(xt,x0),˜βtI), (6)\nwhere ˜µt(xt,x0) :=\n√¯αt−1βt\n1 −¯αt\nx0 +\n√αt(1 −¯αt−1)\n1 −¯αt\nxt and ˜βt := 1 −¯αt−1\n1 −¯αt\nβt (7)\nConsequently, all KL divergences in Eq. (5) are comparisons between Gaussians, so they can be\ncalculated in a Rao-Blackwellized fashion with closed form expressions instead of high variance\nMonte Carlo estimates.\n3 Diffusion models and denoising autoencoders\nDiffusion models might appear to be a restricted class of latent variable models, but they allow a\nlarge number of degrees of freedom in implementation. One must choose the variances βt of the\nforward process and the model architecture and Gaussian distribution parameterization of the reverse\nprocess. To guide our choices, we establish a new explicit connection between diffusion models\nand denoising score matching (Section 3.2) that leads to a simpliﬁed, weighted variational bound\nobjective for diffusion models (Section 3.4). Ultimately, our model design is justiﬁed by simplicity\nand empirical results (Section 4). Our discussion is categorized by the terms of Eq. (5).\n3.1 Forward process and LT\nWe ignore the fact that the forward process variances βt are learnable by reparameterization and\ninstead ﬁx them to constants (see Section 4 for details). Thus, in our implementation, the approximate\nposterior qhas no learnable parameters, so LT is a constant during training and can be ignored.\n3.2 Reverse process and L1:T−1\nNow we discuss our choices in pθ(xt−1|xt) = N(xt−1; µθ(xt,t),Σθ(xt,t)) for 1 <t ≤T. First,\nwe set Σθ(xt,t) = σ2\ntI to untrained time dependent constants. Experimentally, both σ2\nt = βt and\nσ2\nt = ˜βt = 1−¯αt−1\n1−¯αt\nβt had similar results. The ﬁrst choice is optimal for x0 ∼N (0,I), and th", "section": "unknown", "text_embed": "onal form when\nβt are small [ 53]. A notable property of the forward process is that it admits sampling xt at an\narbitrary timestep tin closed form: using the notation αt := 1 −βt and ¯αt := ∏t\ns=1 αs, we have\nq(xt|x0) = N(xt; √¯αtx0,(1 −¯αt)I) (4)\n2\n\n[PAGE 3]\nEfﬁcient training is therefore possible by optimizing random terms of Lwith stochastic gradient\ndescent. Further improvements come from variance reduction by rewriting L(3) as:\nEq\n[\nDKL(q(xT|x0) ∥p(xT))  \nLT\n+\n∑\nt>1\nDKL(q(xt−1|xt,x0) ∥pθ(xt−1|xt))  \nLt−1\n−log pθ(x0|x1)  \nL0\n]\n(5)\n(See Appendix A for details. The labels on the terms are used in Section 3.) Equation (5) uses KL\ndivergence to directly compare pθ(xt−1|xt) against forward process posteriors, which are tractable\nwhen conditioned on x0:\nq(xt−1|xt,x0) = N(xt−1; ˜µt(xt,x0),˜βtI), (6)\nwhere ˜µt(xt,x0) :=\n√¯αt−1βt\n1 −¯αt\nx0 +\n√αt(1 −¯αt−1)\n1 −¯αt\nxt and ˜βt := 1 −¯αt−1\n1 −¯αt\nβt (7)\nConsequently, all KL divergences in Eq. (5) are comparisons between Gaussians, so they can be\ncalculated in a Rao-Blackwellized fashion with closed form expressions instead of high variance\nMonte Carlo estimates.\n3 Diffusion models and denoising autoencoders\nDiffusion models might appear to be a restricted class of latent variable models, but they allow a\nlarge number of degrees of freedom in implementation. One must choose the variances βt of the\nforward process and the model architecture and Gaussian distribution parameterization of the reverse\nprocess. To guide our choices, we establish a new explicit connection between diffusion models\nand denoising score matching (Section 3.2) that leads to a simpliﬁed, weighted variational bound\nobjective for diffusion models (Section 3.4). Ultimately, our model design is justiﬁed by simplicity\nand empirical results (Section 4). Our discussion is categorized by the terms of Eq. (5).\n3.1 Forward process and LT\nWe ignore the fact that the forward process variances βt are learnable by reparameterization and\ninstead ﬁx them to constants (see Section 4 for details). Thus, in our implementation, the approximate\nposterior qhas no learnable parameters, so LT is a constant during training and can be ignored.\n3.2 Reverse process and L1:T−1\nNow we discuss our choices in pθ(xt−1|xt) = N(xt−1; µθ(xt,t),Σθ(xt,t)) for 1 <t ≤T. First,\nwe set Σθ(xt,t) = σ2\ntI to untrained time dependent constants. Experimentally, both σ2\nt = βt and\nσ2\nt = ˜βt = 1−¯αt−1\n1−¯αt\nβt had similar results. The ﬁrst choice is optimal for x0 ∼N (0,I), and th"}
{"chunk_id": "YOUR_0005", "text": ") = N(xt−1; µθ(xt,t),Σθ(xt,t)) for 1 <t ≤T. First,\nwe set Σθ(xt,t) = σ2\ntI to untrained time dependent constants. Experimentally, both σ2\nt = βt and\nσ2\nt = ˜βt = 1−¯αt−1\n1−¯αt\nβt had similar results. The ﬁrst choice is optimal for x0 ∼N (0,I), and the\nsecond is optimal for x0 deterministically set to one point. These are the two extreme choices\ncorresponding to upper and lower bounds on reverse process entropy for data with coordinatewise\nunit variance [53].\nSecond, to represent the mean µθ(xt,t), we propose a speciﬁc parameterization motivated by the\nfollowing analysis of Lt. With pθ(xt−1|xt) = N(xt−1; µθ(xt,t),σ2\ntI), we can write:\nLt−1 = Eq\n[ 1\n2σ2\nt\n∥˜µt(xt,x0) −µθ(xt,t)∥2\n]\n+ C (8)\nwhere Cis a constant that does not depend on θ. So, we see that the most straightforward parameteri-\nzation of µθ is a model that predicts ˜µt, the forward process posterior mean. However, we can expand\nEq. (8) further by reparameterizing Eq. (4) as xt(x0,ϵ) = √¯αtx0 + √1 −¯αtϵ for ϵ ∼N(0,I) and\napplying the forward process posterior formula (7):\nLt−1 −C = Ex0,ϵ\n[\n1\n2σ2\nt\n˜µt\n(\nxt(x0,ϵ), 1√¯αt\n(xt(x0,ϵ) −\n√\n1 −¯αtϵ)\n)\n−µθ(xt(x0,ϵ),t)\n\n2]\n(9)\n= Ex0,ϵ\n[\n1\n2σ2\nt\n\n1√αt\n(\nxt(x0,ϵ) − βt√1 −¯αt\nϵ\n)\n−µθ(xt(x0,ϵ),t)\n\n2]\n(10)\n3\n\n[PAGE 4]\nAlgorithm 1 Training\n1: repeat\n2: x0 ∼q(x0)\n3: t∼Uniform({1,...,T })\n4: ϵ ∼N(0,I)\n5: Take gradient descent step on\n∇θ\nϵ −ϵθ(√¯αtx0 + √1 −¯αtϵ,t)\n2\n6: until converged\nAlgorithm 2 Sampling\n1: xT ∼N(0,I)\n2: for t= T,..., 1 do\n3: z ∼N(0,I) if t> 1, else z = 0\n4: xt−1 = 1√αt\n(\nxt − 1−αt√1−¯αt\nϵθ(xt,t)\n)\n+ σtz\n5: end for\n6: return x0\nEquation (10) reveals that µθ must predict 1√αt\n(\nxt − βt√1−¯αt\nϵ\n)\ngiven xt. Since xt is available as\ninput to the model, we may choose the parameterization\nµθ(xt,t) = ˜µt\n(\nxt, 1√¯αt\n(xt −\n√\n1 −¯αtϵθ(xt))\n)\n= 1√αt\n(\nxt − βt√1 −¯αt\nϵθ(xt,t)\n)\n(11)\nwhere ϵθ is a function approximator intended to predict ϵ from xt. To sample xt−1 ∼pθ(xt−1|xt) is\nto compute xt−1 = 1√αt\n(\nxt − βt√1−¯αt\nϵθ(xt,t)\n)\n+σtz, where z ∼N(0,I). The complete sampling\nprocedure, Algorithm 2, resembles Langevin dynamics with ϵθ as a learned gradient of the data\ndensity. Furthermore, with the parameterization (11), Eq. (10) simpliﬁes to:\nEx0,ϵ\n[ β2\nt\n2σ2\ntαt(1 −¯αt)\nϵ −ϵθ(√¯αtx0 +\n√\n1 −¯αtϵ,t)\n2\n]\n(12)\nwhich resembles denoising score matching over multiple noise scales indexed by t[55]. As Eq. (12)\nis equal to (one term of) the variational bound for the Langevin-like reverse process (11), we see\nthat optimizing an objective resembling", "page_start": 4, "page_end": 4, "char_count": 2499, "text_raw": ") = N(xt−1; µθ(xt,t),Σθ(xt,t)) for 1 <t ≤T. First,\nwe set Σθ(xt,t) = σ2\ntI to untrained time dependent constants. Experimentally, both σ2\nt = βt and\nσ2\nt = ˜βt = 1−¯αt−1\n1−¯αt\nβt had similar results. The ﬁrst choice is optimal for x0 ∼N (0,I), and the\nsecond is optimal for x0 deterministically set to one point. These are the two extreme choices\ncorresponding to upper and lower bounds on reverse process entropy for data with coordinatewise\nunit variance [53].\nSecond, to represent the mean µθ(xt,t), we propose a speciﬁc parameterization motivated by the\nfollowing analysis of Lt. With pθ(xt−1|xt) = N(xt−1; µθ(xt,t),σ2\ntI), we can write:\nLt−1 = Eq\n[ 1\n2σ2\nt\n∥˜µt(xt,x0) −µθ(xt,t)∥2\n]\n+ C (8)\nwhere Cis a constant that does not depend on θ. So, we see that the most straightforward parameteri-\nzation of µθ is a model that predicts ˜µt, the forward process posterior mean. However, we can expand\nEq. (8) further by reparameterizing Eq. (4) as xt(x0,ϵ) = √¯αtx0 + √1 −¯αtϵ for ϵ ∼N(0,I) and\napplying the forward process posterior formula (7):\nLt−1 −C = Ex0,ϵ\n[\n1\n2σ2\nt\n˜µt\n(\nxt(x0,ϵ), 1√¯αt\n(xt(x0,ϵ) −\n√\n1 −¯αtϵ)\n)\n−µθ(xt(x0,ϵ),t)\n\n2]\n(9)\n= Ex0,ϵ\n[\n1\n2σ2\nt\n\n1√αt\n(\nxt(x0,ϵ) − βt√1 −¯αt\nϵ\n)\n−µθ(xt(x0,ϵ),t)\n\n2]\n(10)\n3\n\n[PAGE 4]\nAlgorithm 1 Training\n1: repeat\n2: x0 ∼q(x0)\n3: t∼Uniform({1,...,T })\n4: ϵ ∼N(0,I)\n5: Take gradient descent step on\n∇θ\nϵ −ϵθ(√¯αtx0 + √1 −¯αtϵ,t)\n2\n6: until converged\nAlgorithm 2 Sampling\n1: xT ∼N(0,I)\n2: for t= T,..., 1 do\n3: z ∼N(0,I) if t> 1, else z = 0\n4: xt−1 = 1√αt\n(\nxt − 1−αt√1−¯αt\nϵθ(xt,t)\n)\n+ σtz\n5: end for\n6: return x0\nEquation (10) reveals that µθ must predict 1√αt\n(\nxt − βt√1−¯αt\nϵ\n)\ngiven xt. Since xt is available as\ninput to the model, we may choose the parameterization\nµθ(xt,t) = ˜µt\n(\nxt, 1√¯αt\n(xt −\n√\n1 −¯αtϵθ(xt))\n)\n= 1√αt\n(\nxt − βt√1 −¯αt\nϵθ(xt,t)\n)\n(11)\nwhere ϵθ is a function approximator intended to predict ϵ from xt. To sample xt−1 ∼pθ(xt−1|xt) is\nto compute xt−1 = 1√αt\n(\nxt − βt√1−¯αt\nϵθ(xt,t)\n)\n+σtz, where z ∼N(0,I). The complete sampling\nprocedure, Algorithm 2, resembles Langevin dynamics with ϵθ as a learned gradient of the data\ndensity. Furthermore, with the parameterization (11), Eq. (10) simpliﬁes to:\nEx0,ϵ\n[ β2\nt\n2σ2\ntαt(1 −¯αt)\nϵ −ϵθ(√¯αtx0 +\n√\n1 −¯αtϵ,t)\n2\n]\n(12)\nwhich resembles denoising score matching over multiple noise scales indexed by t[55]. As Eq. (12)\nis equal to (one term of) the variational bound for the Langevin-like reverse process (11), we see\nthat optimizing an objective resembling", "section": "unknown", "text_embed": ") = N(xt−1; µθ(xt,t),Σθ(xt,t)) for 1 <t ≤T. First,\nwe set Σθ(xt,t) = σ2\ntI to untrained time dependent constants. Experimentally, both σ2\nt = βt and\nσ2\nt = ˜βt = 1−¯αt−1\n1−¯αt\nβt had similar results. The ﬁrst choice is optimal for x0 ∼N (0,I), and the\nsecond is optimal for x0 deterministically set to one point. These are the two extreme choices\ncorresponding to upper and lower bounds on reverse process entropy for data with coordinatewise\nunit variance [53].\nSecond, to represent the mean µθ(xt,t), we propose a speciﬁc parameterization motivated by the\nfollowing analysis of Lt. With pθ(xt−1|xt) = N(xt−1; µθ(xt,t),σ2\ntI), we can write:\nLt−1 = Eq\n[ 1\n2σ2\nt\n∥˜µt(xt,x0) −µθ(xt,t)∥2\n]\n+ C (8)\nwhere Cis a constant that does not depend on θ. So, we see that the most straightforward parameteri-\nzation of µθ is a model that predicts ˜µt, the forward process posterior mean. However, we can expand\nEq. (8) further by reparameterizing Eq. (4) as xt(x0,ϵ) = √¯αtx0 + √1 −¯αtϵ for ϵ ∼N(0,I) and\napplying the forward process posterior formula (7):\nLt−1 −C = Ex0,ϵ\n[\n1\n2σ2\nt\n˜µt\n(\nxt(x0,ϵ), 1√¯αt\n(xt(x0,ϵ) −\n√\n1 −¯αtϵ)\n)\n−µθ(xt(x0,ϵ),t)\n\n2]\n(9)\n= Ex0,ϵ\n[\n1\n2σ2\nt\n\n1√αt\n(\nxt(x0,ϵ) − βt√1 −¯αt\nϵ\n)\n−µθ(xt(x0,ϵ),t)\n\n2]\n(10)\n3\n\n[PAGE 4]\nAlgorithm 1 Training\n1: repeat\n2: x0 ∼q(x0)\n3: t∼Uniform({1,...,T })\n4: ϵ ∼N(0,I)\n5: Take gradient descent step on\n∇θ\nϵ −ϵθ(√¯αtx0 + √1 −¯αtϵ,t)\n2\n6: until converged\nAlgorithm 2 Sampling\n1: xT ∼N(0,I)\n2: for t= T,..., 1 do\n3: z ∼N(0,I) if t> 1, else z = 0\n4: xt−1 = 1√αt\n(\nxt − 1−αt√1−¯αt\nϵθ(xt,t)\n)\n+ σtz\n5: end for\n6: return x0\nEquation (10) reveals that µθ must predict 1√αt\n(\nxt − βt√1−¯αt\nϵ\n)\ngiven xt. Since xt is available as\ninput to the model, we may choose the parameterization\nµθ(xt,t) = ˜µt\n(\nxt, 1√¯αt\n(xt −\n√\n1 −¯αtϵθ(xt))\n)\n= 1√αt\n(\nxt − βt√1 −¯αt\nϵθ(xt,t)\n)\n(11)\nwhere ϵθ is a function approximator intended to predict ϵ from xt. To sample xt−1 ∼pθ(xt−1|xt) is\nto compute xt−1 = 1√αt\n(\nxt − βt√1−¯αt\nϵθ(xt,t)\n)\n+σtz, where z ∼N(0,I). The complete sampling\nprocedure, Algorithm 2, resembles Langevin dynamics with ϵθ as a learned gradient of the data\ndensity. Furthermore, with the parameterization (11), Eq. (10) simpliﬁes to:\nEx0,ϵ\n[ β2\nt\n2σ2\ntαt(1 −¯αt)\nϵ −ϵθ(√¯αtx0 +\n√\n1 −¯αtϵ,t)\n2\n]\n(12)\nwhich resembles denoising score matching over multiple noise scales indexed by t[55]. As Eq. (12)\nis equal to (one term of) the variational bound for the Langevin-like reverse process (11), we see\nthat optimizing an objective resembling"}
{"chunk_id": "YOUR_0006", "text": ")\n2\n]\n(12)\nwhich resembles denoising score matching over multiple noise scales indexed by t[55]. As Eq. (12)\nis equal to (one term of) the variational bound for the Langevin-like reverse process (11), we see\nthat optimizing an objective resembling denoising score matching is equivalent to using variational\ninference to ﬁt the ﬁnite-time marginal of a sampling chain resembling Langevin dynamics.\nTo summarize, we can train the reverse process mean function approximator µθ to predict ˜µt, or by\nmodifying its parameterization, we can train it to predict ϵ. (There is also the possibility of predicting\nx0, but we found this to lead to worse sample quality early in our experiments.) We have shown that\nthe ϵ-prediction parameterization both resembles Langevin dynamics and simpliﬁes the diffusion\nmodel’s variational bound to an objective that resembles denoising score matching. Nonetheless,\nit is just another parameterization of pθ(xt−1|xt), so we verify its effectiveness in Section 4 in an\nablation where we compare predicting ϵ against predicting ˜µt.\n3.3 Data scaling, reverse process decoder, and L0\nWe assume that image data consists of integers in {0,1,..., 255}scaled linearly to [−1,1]. This\nensures that the neural network reverse process operates on consistently scaled inputs starting from\nthe standard normal prior p(xT). To obtain discrete log likelihoods, we set the last term of the reverse\nprocess to an independent discrete decoder derived from the Gaussian N(x0; µθ(x1,1),σ2\n1I):\npθ(x0|x1) =\nD∏\ni=1\n∫ δ+(xi\n0)\nδ−(xi\n0)\nN(x; µi\nθ(x1,1),σ2\n1) dx\nδ+(x) =\n{∞ if x= 1\nx+ 1\n255 if x< 1 δ−(x) =\n{−∞ if x= −1\nx− 1\n255 if x> −1\n(13)\nwhere D is the data dimensionality and the i superscript indicates extraction of one coordinate.\n(It would be straightforward to instead incorporate a more powerful decoder like a conditional\nautoregressive model, but we leave that to future work.) Similar to the discretized continuous\ndistributions used in V AE decoders and autoregressive models [34, 52], our choice here ensures that\nthe variational bound is a lossless codelength of discrete data, without need of adding noise to the\ndata or incorporating the Jacobian of the scaling operation into the log likelihood. At the end of\nsampling, we display µθ(x1,1) noiselessly.\n3.4 Simpliﬁed training objective\nWith the reverse process and decoder deﬁned above, the variational bound, consisting of terms derived\nfrom Eqs. (12) and (13), is clearly differentiable with respect to θand is ready to", "page_start": 1, "page_end": 1, "char_count": 2499, "text_raw": ")\n2\n]\n(12)\nwhich resembles denoising score matching over multiple noise scales indexed by t[55]. As Eq. (12)\nis equal to (one term of) the variational bound for the Langevin-like reverse process (11), we see\nthat optimizing an objective resembling denoising score matching is equivalent to using variational\ninference to ﬁt the ﬁnite-time marginal of a sampling chain resembling Langevin dynamics.\nTo summarize, we can train the reverse process mean function approximator µθ to predict ˜µt, or by\nmodifying its parameterization, we can train it to predict ϵ. (There is also the possibility of predicting\nx0, but we found this to lead to worse sample quality early in our experiments.) We have shown that\nthe ϵ-prediction parameterization both resembles Langevin dynamics and simpliﬁes the diffusion\nmodel’s variational bound to an objective that resembles denoising score matching. Nonetheless,\nit is just another parameterization of pθ(xt−1|xt), so we verify its effectiveness in Section 4 in an\nablation where we compare predicting ϵ against predicting ˜µt.\n3.3 Data scaling, reverse process decoder, and L0\nWe assume that image data consists of integers in {0,1,..., 255}scaled linearly to [−1,1]. This\nensures that the neural network reverse process operates on consistently scaled inputs starting from\nthe standard normal prior p(xT). To obtain discrete log likelihoods, we set the last term of the reverse\nprocess to an independent discrete decoder derived from the Gaussian N(x0; µθ(x1,1),σ2\n1I):\npθ(x0|x1) =\nD∏\ni=1\n∫ δ+(xi\n0)\nδ−(xi\n0)\nN(x; µi\nθ(x1,1),σ2\n1) dx\nδ+(x) =\n{∞ if x= 1\nx+ 1\n255 if x< 1 δ−(x) =\n{−∞ if x= −1\nx− 1\n255 if x> −1\n(13)\nwhere D is the data dimensionality and the i superscript indicates extraction of one coordinate.\n(It would be straightforward to instead incorporate a more powerful decoder like a conditional\nautoregressive model, but we leave that to future work.) Similar to the discretized continuous\ndistributions used in V AE decoders and autoregressive models [34, 52], our choice here ensures that\nthe variational bound is a lossless codelength of discrete data, without need of adding noise to the\ndata or incorporating the Jacobian of the scaling operation into the log likelihood. At the end of\nsampling, we display µθ(x1,1) noiselessly.\n3.4 Simpliﬁed training objective\nWith the reverse process and decoder deﬁned above, the variational bound, consisting of terms derived\nfrom Eqs. (12) and (13), is clearly differentiable with respect to θand is ready to", "section": "unknown", "text_embed": ")\n2\n]\n(12)\nwhich resembles denoising score matching over multiple noise scales indexed by t[55]. As Eq. (12)\nis equal to (one term of) the variational bound for the Langevin-like reverse process (11), we see\nthat optimizing an objective resembling denoising score matching is equivalent to using variational\ninference to ﬁt the ﬁnite-time marginal of a sampling chain resembling Langevin dynamics.\nTo summarize, we can train the reverse process mean function approximator µθ to predict ˜µt, or by\nmodifying its parameterization, we can train it to predict ϵ. (There is also the possibility of predicting\nx0, but we found this to lead to worse sample quality early in our experiments.) We have shown that\nthe ϵ-prediction parameterization both resembles Langevin dynamics and simpliﬁes the diffusion\nmodel’s variational bound to an objective that resembles denoising score matching. Nonetheless,\nit is just another parameterization of pθ(xt−1|xt), so we verify its effectiveness in Section 4 in an\nablation where we compare predicting ϵ against predicting ˜µt.\n3.3 Data scaling, reverse process decoder, and L0\nWe assume that image data consists of integers in {0,1,..., 255}scaled linearly to [−1,1]. This\nensures that the neural network reverse process operates on consistently scaled inputs starting from\nthe standard normal prior p(xT). To obtain discrete log likelihoods, we set the last term of the reverse\nprocess to an independent discrete decoder derived from the Gaussian N(x0; µθ(x1,1),σ2\n1I):\npθ(x0|x1) =\nD∏\ni=1\n∫ δ+(xi\n0)\nδ−(xi\n0)\nN(x; µi\nθ(x1,1),σ2\n1) dx\nδ+(x) =\n{∞ if x= 1\nx+ 1\n255 if x< 1 δ−(x) =\n{−∞ if x= −1\nx− 1\n255 if x> −1\n(13)\nwhere D is the data dimensionality and the i superscript indicates extraction of one coordinate.\n(It would be straightforward to instead incorporate a more powerful decoder like a conditional\nautoregressive model, but we leave that to future work.) Similar to the discretized continuous\ndistributions used in V AE decoders and autoregressive models [34, 52], our choice here ensures that\nthe variational bound is a lossless codelength of discrete data, without need of adding noise to the\ndata or incorporating the Jacobian of the scaling operation into the log likelihood. At the end of\nsampling, we display µθ(x1,1) noiselessly.\n3.4 Simpliﬁed training objective\nWith the reverse process and decoder deﬁned above, the variational bound, consisting of terms derived\nfrom Eqs. (12) and (13), is clearly differentiable with respect to θand is ready to"}
{"chunk_id": "YOUR_0008", "text": "standard variational\nbound [18, 22]. In particular, our diffusion process setup in Section 4 causes the simpliﬁed objective\nto down-weight loss terms corresponding to small t. These terms train the network to denoise data\nwith very small amounts of noise, so it is beneﬁcial to down-weight them so that the network can\nfocus on more difﬁcult denoising tasks at larger tterms. We will see in our experiments that this\nreweighting leads to better sample quality.\n4 Experiments\nWe set T = 1000 for all experiments so that the number of neural network evaluations needed\nduring sampling matches previous work [53, 55]. We set the forward process variances to constants\nincreasing linearly from β1 = 10 −4 to βT = 0 .02. These constants were chosen to be small\nrelative to data scaled to [−1,1], ensuring that reverse and forward processes have approximately\nthe same functional form while keeping the signal-to-noise ratio at xT as small as possible (LT =\nDKL(q(xT|x0) ∥N(0,I)) ≈10−5 bits per dimension in our experiments).\nTo represent the reverse process, we use a U-Net backbone similar to an unmasked PixelCNN++ [52,\n48] with group normalization throughout [66]. Parameters are shared across time, which is speciﬁed\nto the network using the Transformer sinusoidal position embedding [60]. We use self-attention at\nthe 16 ×16 feature map resolution [63, 60]. Details are in Appendix B.\n4.1 Sample quality\nTable 1 shows Inception scores, FID scores, and negative log likelihoods (lossless codelengths) on\nCIFAR10. With our FID score of 3.17, our unconditional model achieves better sample quality than\nmost models in the literature, including class conditional models. Our FID score is computed with\nrespect to the training set, as is standard practice; when we compute it with respect to the test set, the\nscore is 5.24, which is still better than many of the training set FID scores in the literature.\n5", "page_start": 1, "page_end": 1, "char_count": 1903, "text_raw": "standard variational\nbound [18, 22]. In particular, our diffusion process setup in Section 4 causes the simpliﬁed objective\nto down-weight loss terms corresponding to small t. These terms train the network to denoise data\nwith very small amounts of noise, so it is beneﬁcial to down-weight them so that the network can\nfocus on more difﬁcult denoising tasks at larger tterms. We will see in our experiments that this\nreweighting leads to better sample quality.\n4 Experiments\nWe set T = 1000 for all experiments so that the number of neural network evaluations needed\nduring sampling matches previous work [53, 55]. We set the forward process variances to constants\nincreasing linearly from β1 = 10 −4 to βT = 0 .02. These constants were chosen to be small\nrelative to data scaled to [−1,1], ensuring that reverse and forward processes have approximately\nthe same functional form while keeping the signal-to-noise ratio at xT as small as possible (LT =\nDKL(q(xT|x0) ∥N(0,I)) ≈10−5 bits per dimension in our experiments).\nTo represent the reverse process, we use a U-Net backbone similar to an unmasked PixelCNN++ [52,\n48] with group normalization throughout [66]. Parameters are shared across time, which is speciﬁed\nto the network using the Transformer sinusoidal position embedding [60]. We use self-attention at\nthe 16 ×16 feature map resolution [63, 60]. Details are in Appendix B.\n4.1 Sample quality\nTable 1 shows Inception scores, FID scores, and negative log likelihoods (lossless codelengths) on\nCIFAR10. With our FID score of 3.17, our unconditional model achieves better sample quality than\nmost models in the literature, including class conditional models. Our FID score is computed with\nrespect to the training set, as is standard practice; when we compute it with respect to the test set, the\nscore is 5.24, which is still better than many of the training set FID scores in the literature.\n5", "section": "unknown", "text_embed": "standard variational\nbound [18, 22]. In particular, our diffusion process setup in Section 4 causes the simpliﬁed objective\nto down-weight loss terms corresponding to small t. These terms train the network to denoise data\nwith very small amounts of noise, so it is beneﬁcial to down-weight them so that the network can\nfocus on more difﬁcult denoising tasks at larger tterms. We will see in our experiments that this\nreweighting leads to better sample quality.\n4 Experiments\nWe set T = 1000 for all experiments so that the number of neural network evaluations needed\nduring sampling matches previous work [53, 55]. We set the forward process variances to constants\nincreasing linearly from β1 = 10 −4 to βT = 0 .02. These constants were chosen to be small\nrelative to data scaled to [−1,1], ensuring that reverse and forward processes have approximately\nthe same functional form while keeping the signal-to-noise ratio at xT as small as possible (LT =\nDKL(q(xT|x0) ∥N(0,I)) ≈10−5 bits per dimension in our experiments).\nTo represent the reverse process, we use a U-Net backbone similar to an unmasked PixelCNN++ [52,\n48] with group normalization throughout [66]. Parameters are shared across time, which is speciﬁed\nto the network using the Transformer sinusoidal position embedding [60]. We use self-attention at\nthe 16 ×16 feature map resolution [63, 60]. Details are in Appendix B.\n4.1 Sample quality\nTable 1 shows Inception scores, FID scores, and negative log likelihoods (lossless codelengths) on\nCIFAR10. With our FID score of 3.17, our unconditional model achieves better sample quality than\nmost models in the literature, including class conditional models. Our FID score is computed with\nrespect to the training set, as is standard practice; when we compute it with respect to the test set, the\nscore is 5.24, which is still better than many of the training set FID scores in the literature.\n5"}
{"chunk_id": "YOUR_0009", "text": "ional models. Our FID score is computed with\nrespect to the training set, as is standard practice; when we compute it with respect to the test set, the\nscore is 5.24, which is still better than many of the training set FID scores in the literature.\n5\n\n[PAGE 6]\nFigure 3: LSUN Church samples. FID=7.89\n Figure 4: LSUN Bedroom samples. FID=4.90\nAlgorithm 3 Sending x0\n1: Send xT ∼q(xT|x0) using p(xT)\n2: for t= T −1,..., 2,1 do\n3: Send xt ∼q(xt|xt+1,x0) using pθ(xt|xt+1)\n4: end for\n5: Send x0 using pθ(x0|x1)\nAlgorithm 4 Receiving\n1: Receive xT using p(xT)\n2: for t= T −1,..., 1,0 do\n3: Receive xt using pθ(xt|xt+1)\n4: end for\n5: return x0\nWe ﬁnd that training our models on the true variational bound yields better codelengths than training\non the simpliﬁed objective, as expected, but the latter yields the best sample quality. See Fig. 1 for\nCIFAR10 and CelebA-HQ 256 ×256 samples, Fig. 3 and Fig. 4 for LSUN 256 ×256 samples [71],\nand Appendix D for more.\n4.2 Reverse process parameterization and training objective ablation\nIn Table 2, we show the sample quality effects of reverse process parameterizations and training\nobjectives (Section 3.2). We ﬁnd that the baseline option of predicting ˜µ works well only when\ntrained on the true variational bound instead of unweighted mean squared error, a simpliﬁed objective\nakin to Eq. (14). We also see that learning reverse process variances (by incorporating a parameterized\ndiagonal Σθ(xt) into the variational bound) leads to unstable training and poorer sample quality\ncompared to ﬁxed variances. Predicting ϵ, as we proposed, performs approximately as well as\npredicting ˜µ when trained on the variational bound with ﬁxed variances, but much better when trained\nwith our simpliﬁed objective.\n4.3 Progressive coding\nTable 1 also shows the codelengths of our CIFAR10 models. The gap between train and test is at\nmost 0.03 bits per dimension, which is comparable to the gaps reported with other likelihood-based\nmodels and indicates that our diffusion model is not overﬁtting (see Appendix D for nearest neighbor\nvisualizations). Still, while our lossless codelengths are better than the large estimates reported for\nenergy based models and score matching using annealed importance sampling [ 11], they are not\ncompetitive with other types of likelihood-based generative models [7].\nSince our samples are nonetheless of high quality, we conclude that diffusion models have an inductive\nbias that makes them excellent lossy compressors. Treating th", "page_start": 6, "page_end": 6, "char_count": 2500, "text_raw": "ional models. Our FID score is computed with\nrespect to the training set, as is standard practice; when we compute it with respect to the test set, the\nscore is 5.24, which is still better than many of the training set FID scores in the literature.\n5\n\n[PAGE 6]\nFigure 3: LSUN Church samples. FID=7.89\n Figure 4: LSUN Bedroom samples. FID=4.90\nAlgorithm 3 Sending x0\n1: Send xT ∼q(xT|x0) using p(xT)\n2: for t= T −1,..., 2,1 do\n3: Send xt ∼q(xt|xt+1,x0) using pθ(xt|xt+1)\n4: end for\n5: Send x0 using pθ(x0|x1)\nAlgorithm 4 Receiving\n1: Receive xT using p(xT)\n2: for t= T −1,..., 1,0 do\n3: Receive xt using pθ(xt|xt+1)\n4: end for\n5: return x0\nWe ﬁnd that training our models on the true variational bound yields better codelengths than training\non the simpliﬁed objective, as expected, but the latter yields the best sample quality. See Fig. 1 for\nCIFAR10 and CelebA-HQ 256 ×256 samples, Fig. 3 and Fig. 4 for LSUN 256 ×256 samples [71],\nand Appendix D for more.\n4.2 Reverse process parameterization and training objective ablation\nIn Table 2, we show the sample quality effects of reverse process parameterizations and training\nobjectives (Section 3.2). We ﬁnd that the baseline option of predicting ˜µ works well only when\ntrained on the true variational bound instead of unweighted mean squared error, a simpliﬁed objective\nakin to Eq. (14). We also see that learning reverse process variances (by incorporating a parameterized\ndiagonal Σθ(xt) into the variational bound) leads to unstable training and poorer sample quality\ncompared to ﬁxed variances. Predicting ϵ, as we proposed, performs approximately as well as\npredicting ˜µ when trained on the variational bound with ﬁxed variances, but much better when trained\nwith our simpliﬁed objective.\n4.3 Progressive coding\nTable 1 also shows the codelengths of our CIFAR10 models. The gap between train and test is at\nmost 0.03 bits per dimension, which is comparable to the gaps reported with other likelihood-based\nmodels and indicates that our diffusion model is not overﬁtting (see Appendix D for nearest neighbor\nvisualizations). Still, while our lossless codelengths are better than the large estimates reported for\nenergy based models and score matching using annealed importance sampling [ 11], they are not\ncompetitive with other types of likelihood-based generative models [7].\nSince our samples are nonetheless of high quality, we conclude that diffusion models have an inductive\nbias that makes them excellent lossy compressors. Treating th", "section": "unknown", "text_embed": "ional models. Our FID score is computed with\nrespect to the training set, as is standard practice; when we compute it with respect to the test set, the\nscore is 5.24, which is still better than many of the training set FID scores in the literature.\n5\n\n[PAGE 6]\nFigure 3: LSUN Church samples. FID=7.89\n Figure 4: LSUN Bedroom samples. FID=4.90\nAlgorithm 3 Sending x0\n1: Send xT ∼q(xT|x0) using p(xT)\n2: for t= T −1,..., 2,1 do\n3: Send xt ∼q(xt|xt+1,x0) using pθ(xt|xt+1)\n4: end for\n5: Send x0 using pθ(x0|x1)\nAlgorithm 4 Receiving\n1: Receive xT using p(xT)\n2: for t= T −1,..., 1,0 do\n3: Receive xt using pθ(xt|xt+1)\n4: end for\n5: return x0\nWe ﬁnd that training our models on the true variational bound yields better codelengths than training\non the simpliﬁed objective, as expected, but the latter yields the best sample quality. See Fig. 1 for\nCIFAR10 and CelebA-HQ 256 ×256 samples, Fig. 3 and Fig. 4 for LSUN 256 ×256 samples [71],\nand Appendix D for more.\n4.2 Reverse process parameterization and training objective ablation\nIn Table 2, we show the sample quality effects of reverse process parameterizations and training\nobjectives (Section 3.2). We ﬁnd that the baseline option of predicting ˜µ works well only when\ntrained on the true variational bound instead of unweighted mean squared error, a simpliﬁed objective\nakin to Eq. (14). We also see that learning reverse process variances (by incorporating a parameterized\ndiagonal Σθ(xt) into the variational bound) leads to unstable training and poorer sample quality\ncompared to ﬁxed variances. Predicting ϵ, as we proposed, performs approximately as well as\npredicting ˜µ when trained on the variational bound with ﬁxed variances, but much better when trained\nwith our simpliﬁed objective.\n4.3 Progressive coding\nTable 1 also shows the codelengths of our CIFAR10 models. The gap between train and test is at\nmost 0.03 bits per dimension, which is comparable to the gaps reported with other likelihood-based\nmodels and indicates that our diffusion model is not overﬁtting (see Appendix D for nearest neighbor\nvisualizations). Still, while our lossless codelengths are better than the large estimates reported for\nenergy based models and score matching using annealed importance sampling [ 11], they are not\ncompetitive with other types of likelihood-based generative models [7].\nSince our samples are nonetheless of high quality, we conclude that diffusion models have an inductive\nbias that makes them excellent lossy compressors. Treating th"}
{"chunk_id": "YOUR_0010", "text": "they are not\ncompetitive with other types of likelihood-based generative models [7].\nSince our samples are nonetheless of high quality, we conclude that diffusion models have an inductive\nbias that makes them excellent lossy compressors. Treating the variational bound termsL1 +···+LT\nas rate and L0 as distortion, our CIFAR10 model with the highest quality samples has a rate of 1.78\nbits/dim and a distortion of 1.97 bits/dim, which amounts to a root mean squared error of 0.95 on a\nscale from 0 to 255. More than half of the lossless codelength describes imperceptible distortions.\nProgressive lossy compression We can probe further into the rate-distortion behavior of our model\nby introducing a progressive lossy code that mirrors the form of Eq. (5): see Algorithms 3 and 4,\nwhich assume access to a procedure, such as minimal random coding [19, 20], that can transmit a\nsample x ∼q(x) using approximately DKL(q(x) ∥p(x)) bits on average for any distributions pand\nq, for which onlypis available to the receiver beforehand. When applied tox0 ∼q(x0), Algorithms 3\nand 4 transmit xT,..., x0 in sequence using a total expected codelength equal to Eq. (5). The receiver,\n6\n\n[PAGE 7]\nat any time t, has the partial information xt fully available and can progressively estimate:\nx0 ≈ˆx0 =\n(\nxt −\n√\n1 −¯αtϵθ(xt)\n)\n/√¯αt (15)\ndue to Eq. (4). (A stochastic reconstruction x0 ∼pθ(x0|xt) is also valid, but we do not consider\nit here because it makes distortion more difﬁcult to evaluate.) Figure 5 shows the resulting rate-\ndistortion plot on the CIFAR10 test set. At each time t, the distortion is calculated as the root mean\nsquared error\n√\n∥x0 −ˆx0∥2/D, and the rate is calculated as the cumulative number of bits received\nso far at time t. The distortion decreases steeply in the low-rate region of the rate-distortion plot,\nindicating that the majority of the bits are indeed allocated to imperceptible distortions.\n0 200 400 600 800 1,000\n0\n20\n40\n60\n80\nReverse process steps (T −t)\nDistortion (RMSE)\n0 200 400 600 800 1,000\n0\n0.5\n1\n1.5\nReverse process steps (T −t)\nRate (bits/dim)\n0 0.5 1 1.5\n0\n20\n40\n60\n80\nRate (bits/dim)\nDistortion (RMSE)\nFigure 5: Unconditional CIFAR10 test set rate-distortion vs. time. Distortion is measured in root mean squared\nerror on a [0,255] scale. See Table 4 for details.\nProgressive generation We also run a progressive unconditional generation process given by\nprogressive decompression from random bits. In other words, we predict the result of the reverse\nprocess", "page_start": 7, "page_end": 7, "char_count": 2500, "text_raw": "they are not\ncompetitive with other types of likelihood-based generative models [7].\nSince our samples are nonetheless of high quality, we conclude that diffusion models have an inductive\nbias that makes them excellent lossy compressors. Treating the variational bound termsL1 +···+LT\nas rate and L0 as distortion, our CIFAR10 model with the highest quality samples has a rate of 1.78\nbits/dim and a distortion of 1.97 bits/dim, which amounts to a root mean squared error of 0.95 on a\nscale from 0 to 255. More than half of the lossless codelength describes imperceptible distortions.\nProgressive lossy compression We can probe further into the rate-distortion behavior of our model\nby introducing a progressive lossy code that mirrors the form of Eq. (5): see Algorithms 3 and 4,\nwhich assume access to a procedure, such as minimal random coding [19, 20], that can transmit a\nsample x ∼q(x) using approximately DKL(q(x) ∥p(x)) bits on average for any distributions pand\nq, for which onlypis available to the receiver beforehand. When applied tox0 ∼q(x0), Algorithms 3\nand 4 transmit xT,..., x0 in sequence using a total expected codelength equal to Eq. (5). The receiver,\n6\n\n[PAGE 7]\nat any time t, has the partial information xt fully available and can progressively estimate:\nx0 ≈ˆx0 =\n(\nxt −\n√\n1 −¯αtϵθ(xt)\n)\n/√¯αt (15)\ndue to Eq. (4). (A stochastic reconstruction x0 ∼pθ(x0|xt) is also valid, but we do not consider\nit here because it makes distortion more difﬁcult to evaluate.) Figure 5 shows the resulting rate-\ndistortion plot on the CIFAR10 test set. At each time t, the distortion is calculated as the root mean\nsquared error\n√\n∥x0 −ˆx0∥2/D, and the rate is calculated as the cumulative number of bits received\nso far at time t. The distortion decreases steeply in the low-rate region of the rate-distortion plot,\nindicating that the majority of the bits are indeed allocated to imperceptible distortions.\n0 200 400 600 800 1,000\n0\n20\n40\n60\n80\nReverse process steps (T −t)\nDistortion (RMSE)\n0 200 400 600 800 1,000\n0\n0.5\n1\n1.5\nReverse process steps (T −t)\nRate (bits/dim)\n0 0.5 1 1.5\n0\n20\n40\n60\n80\nRate (bits/dim)\nDistortion (RMSE)\nFigure 5: Unconditional CIFAR10 test set rate-distortion vs. time. Distortion is measured in root mean squared\nerror on a [0,255] scale. See Table 4 for details.\nProgressive generation We also run a progressive unconditional generation process given by\nprogressive decompression from random bits. In other words, we predict the result of the reverse\nprocess", "section": "unknown", "text_embed": "they are not\ncompetitive with other types of likelihood-based generative models [7].\nSince our samples are nonetheless of high quality, we conclude that diffusion models have an inductive\nbias that makes them excellent lossy compressors. Treating the variational bound termsL1 +···+LT\nas rate and L0 as distortion, our CIFAR10 model with the highest quality samples has a rate of 1.78\nbits/dim and a distortion of 1.97 bits/dim, which amounts to a root mean squared error of 0.95 on a\nscale from 0 to 255. More than half of the lossless codelength describes imperceptible distortions.\nProgressive lossy compression We can probe further into the rate-distortion behavior of our model\nby introducing a progressive lossy code that mirrors the form of Eq. (5): see Algorithms 3 and 4,\nwhich assume access to a procedure, such as minimal random coding [19, 20], that can transmit a\nsample x ∼q(x) using approximately DKL(q(x) ∥p(x)) bits on average for any distributions pand\nq, for which onlypis available to the receiver beforehand. When applied tox0 ∼q(x0), Algorithms 3\nand 4 transmit xT,..., x0 in sequence using a total expected codelength equal to Eq. (5). The receiver,\n6\n\n[PAGE 7]\nat any time t, has the partial information xt fully available and can progressively estimate:\nx0 ≈ˆx0 =\n(\nxt −\n√\n1 −¯αtϵθ(xt)\n)\n/√¯αt (15)\ndue to Eq. (4). (A stochastic reconstruction x0 ∼pθ(x0|xt) is also valid, but we do not consider\nit here because it makes distortion more difﬁcult to evaluate.) Figure 5 shows the resulting rate-\ndistortion plot on the CIFAR10 test set. At each time t, the distortion is calculated as the root mean\nsquared error\n√\n∥x0 −ˆx0∥2/D, and the rate is calculated as the cumulative number of bits received\nso far at time t. The distortion decreases steeply in the low-rate region of the rate-distortion plot,\nindicating that the majority of the bits are indeed allocated to imperceptible distortions.\n0 200 400 600 800 1,000\n0\n20\n40\n60\n80\nReverse process steps (T −t)\nDistortion (RMSE)\n0 200 400 600 800 1,000\n0\n0.5\n1\n1.5\nReverse process steps (T −t)\nRate (bits/dim)\n0 0.5 1 1.5\n0\n20\n40\n60\n80\nRate (bits/dim)\nDistortion (RMSE)\nFigure 5: Unconditional CIFAR10 test set rate-distortion vs. time. Distortion is measured in root mean squared\nerror on a [0,255] scale. See Table 4 for details.\nProgressive generation We also run a progressive unconditional generation process given by\nprogressive decompression from random bits. In other words, we predict the result of the reverse\nprocess"}
{"chunk_id": "YOUR_0011", "text": "red\nerror on a [0,255] scale. See Table 4 for details.\nProgressive generation We also run a progressive unconditional generation process given by\nprogressive decompression from random bits. In other words, we predict the result of the reverse\nprocess, ˆx0, while sampling from the reverse process using Algorithm 2. Figures 6 and 10 show the\nresulting sample quality of ˆx0 over the course of the reverse process. Large scale image features\nappear ﬁrst and details appear last. Figure 7 shows stochastic predictions x0 ∼pθ(x0|xt) with xt\nfrozen for various t. When tis small, all but ﬁne details are preserved, and when tis large, only large\nscale features are preserved. Perhaps these are hints of conceptual compression [18].\nFigure 6: Unconditional CIFAR10 progressive generation (ˆx0 over time, from left to right). Extended samples\nand sample quality metrics over time in the appendix (Figs. 10 and 14).\nFigure 7: When conditioned on the same latent, CelebA-HQ 256 ×256 samples share high-level attributes.\nBottom-right quadrants are xt, and other quadrants are samples from pθ(x0|xt).\nConnection to autoregressive decoding Note that the variational bound (5) can be rewritten as:\nL= DKL(q(xT) ∥p(xT)) + Eq\n[∑\nt≥1\nDKL(q(xt−1|xt) ∥pθ(xt−1|xt))\n]\n+ H(x0) (16)\n(See Appendix A for a derivation.) Now consider setting the diffusion process length T to the\ndimensionality of the data, deﬁning the forward process so that q(xt|x0) places all probability mass\non x0 with the ﬁrst tcoordinates masked out (i.e. q(xt|xt−1) masks out the tth coordinate), setting\np(xT) to place all mass on a blank image, and, for the sake of argument, taking pθ(xt−1|xt) to\n7", "page_start": 1, "page_end": 1, "char_count": 1654, "text_raw": "red\nerror on a [0,255] scale. See Table 4 for details.\nProgressive generation We also run a progressive unconditional generation process given by\nprogressive decompression from random bits. In other words, we predict the result of the reverse\nprocess, ˆx0, while sampling from the reverse process using Algorithm 2. Figures 6 and 10 show the\nresulting sample quality of ˆx0 over the course of the reverse process. Large scale image features\nappear ﬁrst and details appear last. Figure 7 shows stochastic predictions x0 ∼pθ(x0|xt) with xt\nfrozen for various t. When tis small, all but ﬁne details are preserved, and when tis large, only large\nscale features are preserved. Perhaps these are hints of conceptual compression [18].\nFigure 6: Unconditional CIFAR10 progressive generation (ˆx0 over time, from left to right). Extended samples\nand sample quality metrics over time in the appendix (Figs. 10 and 14).\nFigure 7: When conditioned on the same latent, CelebA-HQ 256 ×256 samples share high-level attributes.\nBottom-right quadrants are xt, and other quadrants are samples from pθ(x0|xt).\nConnection to autoregressive decoding Note that the variational bound (5) can be rewritten as:\nL= DKL(q(xT) ∥p(xT)) + Eq\n[∑\nt≥1\nDKL(q(xt−1|xt) ∥pθ(xt−1|xt))\n]\n+ H(x0) (16)\n(See Appendix A for a derivation.) Now consider setting the diffusion process length T to the\ndimensionality of the data, deﬁning the forward process so that q(xt|x0) places all probability mass\non x0 with the ﬁrst tcoordinates masked out (i.e. q(xt|xt−1) masks out the tth coordinate), setting\np(xT) to place all mass on a blank image, and, for the sake of argument, taking pθ(xt−1|xt) to\n7", "section": "unknown", "text_embed": "red\nerror on a [0,255] scale. See Table 4 for details.\nProgressive generation We also run a progressive unconditional generation process given by\nprogressive decompression from random bits. In other words, we predict the result of the reverse\nprocess, ˆx0, while sampling from the reverse process using Algorithm 2. Figures 6 and 10 show the\nresulting sample quality of ˆx0 over the course of the reverse process. Large scale image features\nappear ﬁrst and details appear last. Figure 7 shows stochastic predictions x0 ∼pθ(x0|xt) with xt\nfrozen for various t. When tis small, all but ﬁne details are preserved, and when tis large, only large\nscale features are preserved. Perhaps these are hints of conceptual compression [18].\nFigure 6: Unconditional CIFAR10 progressive generation (ˆx0 over time, from left to right). Extended samples\nand sample quality metrics over time in the appendix (Figs. 10 and 14).\nFigure 7: When conditioned on the same latent, CelebA-HQ 256 ×256 samples share high-level attributes.\nBottom-right quadrants are xt, and other quadrants are samples from pθ(x0|xt).\nConnection to autoregressive decoding Note that the variational bound (5) can be rewritten as:\nL= DKL(q(xT) ∥p(xT)) + Eq\n[∑\nt≥1\nDKL(q(xt−1|xt) ∥pθ(xt−1|xt))\n]\n+ H(x0) (16)\n(See Appendix A for a derivation.) Now consider setting the diffusion process length T to the\ndimensionality of the data, deﬁning the forward process so that q(xt|x0) places all probability mass\non x0 with the ﬁrst tcoordinates masked out (i.e. q(xt|xt−1) masks out the tth coordinate), setting\np(xT) to place all mass on a blank image, and, for the sake of argument, taking pθ(xt−1|xt) to\n7"}
{"chunk_id": "YOUR_0012", "text": "process so that q(xt|x0) places all probability mass\non x0 with the ﬁrst tcoordinates masked out (i.e. q(xt|xt−1) masks out the tth coordinate), setting\np(xT) to place all mass on a blank image, and, for the sake of argument, taking pθ(xt−1|xt) to\n7\n\n[PAGE 8]\nFigure 8: Interpolations of CelebA-HQ 256x256 images with 500 timesteps of diffusion.\nbe a fully expressive conditional distribution. With these choices, DKL(q(xT) ∥p(xT)) = 0, and\nminimizing DKL(q(xt−1|xt) ∥pθ(xt−1|xt)) trains pθ to copy coordinates t+ 1,...,T unchanged\nand to predict the tth coordinate given t+ 1,...,T . Thus, training pθ with this particular diffusion is\ntraining an autoregressive model.\nWe can therefore interpret the Gaussian diffusion model (2) as a kind of autoregressive model with\na generalized bit ordering that cannot be expressed by reordering data coordinates. Prior work has\nshown that such reorderings introduce inductive biases that have an impact on sample quality [38],\nso we speculate that the Gaussian diffusion serves a similar purpose, perhaps to greater effect since\nGaussian noise might be more natural to add to images compared to masking noise. Moreover, the\nGaussian diffusion length is not restricted to equal the data dimension; for instance, we useT = 1000,\nwhich is less than the dimension of the 32 ×32 ×3 or 256 ×256 ×3 images in our experiments.\nGaussian diffusions can be made shorter for fast sampling or longer for model expressiveness.\n4.4 Interpolation\nWe can interpolate source images x0,x′\n0 ∼q(x0) in latent space using q as a stochastic encoder,\nxt,x′\nt ∼q(xt|x0), then decoding the linearly interpolated latent ¯xt = (1 −λ)x0 + λx′\n0 into image\nspace by the reverse process, ¯x0 ∼p(x0|¯xt). In effect, we use the reverse process to remove\nartifacts from linearly interpolating corrupted versions of the source images, as depicted in Fig. 8\n(left). We ﬁxed the noise for different values of λ so xt and x′\nt remain the same. Fig. 8 (right)\nshows interpolations and reconstructions of original CelebA-HQ 256 ×256 images (t= 500). The\nreverse process produces high-quality reconstructions, and plausible interpolations that smoothly\nvary attributes such as pose, skin tone, hairstyle, expression and background, but not eyewear. Larger\ntresults in coarser and more varied interpolations, with novel samples at t= 1000 (Appendix Fig. 9).\n5 Related Work\nWhile diffusion models might resemble ﬂows [ 9, 46, 10, 32, 5, 16, 23] and V AEs [33, 47, 37],\ndiffusion models are designed so", "page_start": 8, "page_end": 8, "char_count": 2500, "text_raw": "process so that q(xt|x0) places all probability mass\non x0 with the ﬁrst tcoordinates masked out (i.e. q(xt|xt−1) masks out the tth coordinate), setting\np(xT) to place all mass on a blank image, and, for the sake of argument, taking pθ(xt−1|xt) to\n7\n\n[PAGE 8]\nFigure 8: Interpolations of CelebA-HQ 256x256 images with 500 timesteps of diffusion.\nbe a fully expressive conditional distribution. With these choices, DKL(q(xT) ∥p(xT)) = 0, and\nminimizing DKL(q(xt−1|xt) ∥pθ(xt−1|xt)) trains pθ to copy coordinates t+ 1,...,T unchanged\nand to predict the tth coordinate given t+ 1,...,T . Thus, training pθ with this particular diffusion is\ntraining an autoregressive model.\nWe can therefore interpret the Gaussian diffusion model (2) as a kind of autoregressive model with\na generalized bit ordering that cannot be expressed by reordering data coordinates. Prior work has\nshown that such reorderings introduce inductive biases that have an impact on sample quality [38],\nso we speculate that the Gaussian diffusion serves a similar purpose, perhaps to greater effect since\nGaussian noise might be more natural to add to images compared to masking noise. Moreover, the\nGaussian diffusion length is not restricted to equal the data dimension; for instance, we useT = 1000,\nwhich is less than the dimension of the 32 ×32 ×3 or 256 ×256 ×3 images in our experiments.\nGaussian diffusions can be made shorter for fast sampling or longer for model expressiveness.\n4.4 Interpolation\nWe can interpolate source images x0,x′\n0 ∼q(x0) in latent space using q as a stochastic encoder,\nxt,x′\nt ∼q(xt|x0), then decoding the linearly interpolated latent ¯xt = (1 −λ)x0 + λx′\n0 into image\nspace by the reverse process, ¯x0 ∼p(x0|¯xt). In effect, we use the reverse process to remove\nartifacts from linearly interpolating corrupted versions of the source images, as depicted in Fig. 8\n(left). We ﬁxed the noise for different values of λ so xt and x′\nt remain the same. Fig. 8 (right)\nshows interpolations and reconstructions of original CelebA-HQ 256 ×256 images (t= 500). The\nreverse process produces high-quality reconstructions, and plausible interpolations that smoothly\nvary attributes such as pose, skin tone, hairstyle, expression and background, but not eyewear. Larger\ntresults in coarser and more varied interpolations, with novel samples at t= 1000 (Appendix Fig. 9).\n5 Related Work\nWhile diffusion models might resemble ﬂows [ 9, 46, 10, 32, 5, 16, 23] and V AEs [33, 47, 37],\ndiffusion models are designed so", "section": "related_work", "text_embed": "process so that q(xt|x0) places all probability mass\non x0 with the ﬁrst tcoordinates masked out (i.e. q(xt|xt−1) masks out the tth coordinate), setting\np(xT) to place all mass on a blank image, and, for the sake of argument, taking pθ(xt−1|xt) to\n7\n\n[PAGE 8]\nFigure 8: Interpolations of CelebA-HQ 256x256 images with 500 timesteps of diffusion.\nbe a fully expressive conditional distribution. With these choices, DKL(q(xT) ∥p(xT)) = 0, and\nminimizing DKL(q(xt−1|xt) ∥pθ(xt−1|xt)) trains pθ to copy coordinates t+ 1,...,T unchanged\nand to predict the tth coordinate given t+ 1,...,T . Thus, training pθ with this particular diffusion is\ntraining an autoregressive model.\nWe can therefore interpret the Gaussian diffusion model (2) as a kind of autoregressive model with\na generalized bit ordering that cannot be expressed by reordering data coordinates. Prior work has\nshown that such reorderings introduce inductive biases that have an impact on sample quality [38],\nso we speculate that the Gaussian diffusion serves a similar purpose, perhaps to greater effect since\nGaussian noise might be more natural to add to images compared to masking noise. Moreover, the\nGaussian diffusion length is not restricted to equal the data dimension; for instance, we useT = 1000,\nwhich is less than the dimension of the 32 ×32 ×3 or 256 ×256 ×3 images in our experiments.\nGaussian diffusions can be made shorter for fast sampling or longer for model expressiveness.\n4.4 Interpolation\nWe can interpolate source images x0,x′\n0 ∼q(x0) in latent space using q as a stochastic encoder,\nxt,x′\nt ∼q(xt|x0), then decoding the linearly interpolated latent ¯xt = (1 −λ)x0 + λx′\n0 into image\nspace by the reverse process, ¯x0 ∼p(x0|¯xt). In effect, we use the reverse process to remove\nartifacts from linearly interpolating corrupted versions of the source images, as depicted in Fig. 8\n(left). We ﬁxed the noise for different values of λ so xt and x′\nt remain the same. Fig. 8 (right)\nshows interpolations and reconstructions of original CelebA-HQ 256 ×256 images (t= 500). The\nreverse process produces high-quality reconstructions, and plausible interpolations that smoothly\nvary attributes such as pose, skin tone, hairstyle, expression and background, but not eyewear. Larger\ntresults in coarser and more varied interpolations, with novel samples at t= 1000 (Appendix Fig. 9).\n5 Related Work\nWhile diffusion models might resemble ﬂows [ 9, 46, 10, 32, 5, 16, 23] and V AEs [33, 47, 37],\ndiffusion models are designed so"}
{"chunk_id": "YOUR_0013", "text": "Larger\ntresults in coarser and more varied interpolations, with novel samples at t= 1000 (Appendix Fig. 9).\n5 Related Work\nWhile diffusion models might resemble ﬂows [ 9, 46, 10, 32, 5, 16, 23] and V AEs [33, 47, 37],\ndiffusion models are designed so that qhas no parameters and the top-level latent xT has nearly zero\nmutual information with the data x0. Our ϵ-prediction reverse process parameterization establishes a\nconnection between diffusion models and denoising score matching over multiple noise levels with\nannealed Langevin dynamics for sampling [55, 56]. Diffusion models, however, admit straightforward\nlog likelihood evaluation, and the training procedure explicitly trains the Langevin dynamics sampler\nusing variational inference (see Appendix C for details). The connection also has the reverse\nimplication that a certain weighted form of denoising score matching is the same as variational\ninference to train a Langevin-like sampler. Other methods for learning transition operators of Markov\nchains include infusion training [2], variational walkback [15], generative stochastic networks [1],\nand others [50, 54, 36, 42, 35, 65].\nBy the known connection between score matching and energy-based modeling, our work could have\nimplications for other recent work on energy-based models [67–69, 12, 70, 13, 11, 41, 17, 8]. Our\nrate-distortion curves are computed over time in one evaluation of the variational bound, reminiscent\nof how rate-distortion curves can be computed over distortion penalties in one run of annealed\nimportance sampling [24]. Our progressive decoding argument can be seen in convolutional DRAW\nand related models [ 18, 40] and may also lead to more general designs for subscale orderings or\nsampling strategies for autoregressive models [38, 64].\n8", "page_start": 1, "page_end": 1, "char_count": 1785, "text_raw": "Larger\ntresults in coarser and more varied interpolations, with novel samples at t= 1000 (Appendix Fig. 9).\n5 Related Work\nWhile diffusion models might resemble ﬂows [ 9, 46, 10, 32, 5, 16, 23] and V AEs [33, 47, 37],\ndiffusion models are designed so that qhas no parameters and the top-level latent xT has nearly zero\nmutual information with the data x0. Our ϵ-prediction reverse process parameterization establishes a\nconnection between diffusion models and denoising score matching over multiple noise levels with\nannealed Langevin dynamics for sampling [55, 56]. Diffusion models, however, admit straightforward\nlog likelihood evaluation, and the training procedure explicitly trains the Langevin dynamics sampler\nusing variational inference (see Appendix C for details). The connection also has the reverse\nimplication that a certain weighted form of denoising score matching is the same as variational\ninference to train a Langevin-like sampler. Other methods for learning transition operators of Markov\nchains include infusion training [2], variational walkback [15], generative stochastic networks [1],\nand others [50, 54, 36, 42, 35, 65].\nBy the known connection between score matching and energy-based modeling, our work could have\nimplications for other recent work on energy-based models [67–69, 12, 70, 13, 11, 41, 17, 8]. Our\nrate-distortion curves are computed over time in one evaluation of the variational bound, reminiscent\nof how rate-distortion curves can be computed over distortion penalties in one run of annealed\nimportance sampling [24]. Our progressive decoding argument can be seen in convolutional DRAW\nand related models [ 18, 40] and may also lead to more general designs for subscale orderings or\nsampling strategies for autoregressive models [38, 64].\n8", "section": "related_work", "text_embed": "Larger\ntresults in coarser and more varied interpolations, with novel samples at t= 1000 (Appendix Fig. 9).\n5 Related Work\nWhile diffusion models might resemble ﬂows [ 9, 46, 10, 32, 5, 16, 23] and V AEs [33, 47, 37],\ndiffusion models are designed so that qhas no parameters and the top-level latent xT has nearly zero\nmutual information with the data x0. Our ϵ-prediction reverse process parameterization establishes a\nconnection between diffusion models and denoising score matching over multiple noise levels with\nannealed Langevin dynamics for sampling [55, 56]. Diffusion models, however, admit straightforward\nlog likelihood evaluation, and the training procedure explicitly trains the Langevin dynamics sampler\nusing variational inference (see Appendix C for details). The connection also has the reverse\nimplication that a certain weighted form of denoising score matching is the same as variational\ninference to train a Langevin-like sampler. Other methods for learning transition operators of Markov\nchains include infusion training [2], variational walkback [15], generative stochastic networks [1],\nand others [50, 54, 36, 42, 35, 65].\nBy the known connection between score matching and energy-based modeling, our work could have\nimplications for other recent work on energy-based models [67–69, 12, 70, 13, 11, 41, 17, 8]. Our\nrate-distortion curves are computed over time in one evaluation of the variational bound, reminiscent\nof how rate-distortion curves can be computed over distortion penalties in one run of annealed\nimportance sampling [24]. Our progressive decoding argument can be seen in convolutional DRAW\nand related models [ 18, 40] and may also lead to more general designs for subscale orderings or\nsampling strategies for autoregressive models [38, 64].\n8"}
{"chunk_id": "YOUR_0014", "text": "d\nimportance sampling [24]. Our progressive decoding argument can be seen in convolutional DRAW\nand related models [ 18, 40] and may also lead to more general designs for subscale orderings or\nsampling strategies for autoregressive models [38, 64].\n8\n\n[PAGE 9]\n6 Conclusion\nWe have presented high quality image samples using diffusion models, and we have found connections\namong diffusion models and variational inference for training Markov chains, denoising score\nmatching and annealed Langevin dynamics (and energy-based models by extension), autoregressive\nmodels, and progressive lossy compression. Since diffusion models seem to have excellent inductive\nbiases for image data, we look forward to investigating their utility in other data modalities and as\ncomponents in other types of generative models and machine learning systems.\nBroader Impact\nOur work on diffusion models takes on a similar scope as existing work on other types of deep\ngenerative models, such as efforts to improve the sample quality of GANs, ﬂows, autoregressive\nmodels, and so forth. Our paper represents progress in making diffusion models a generally useful\ntool in this family of techniques, so it may serve to amplify any impacts that generative models have\nhad (and will have) on the broader world.\nUnfortunately, there are numerous well-known malicious uses of generative models. Sample gen-\neration techniques can be employed to produce fake images and videos of high proﬁle ﬁgures for\npolitical purposes. While fake images were manually created long before software tools were avail-\nable, generative models such as ours make the process easier. Fortunately, CNN-generated images\ncurrently have subtle ﬂaws that allow detection [62], but improvements in generative models may\nmake this more difﬁcult. Generative models also reﬂect the biases in the datasets on which they\nare trained. As many large datasets are collected from the internet by automated systems, it can be\ndifﬁcult to remove these biases, especially when the images are unlabeled. If samples from generative\nmodels trained on these datasets proliferate throughout the internet, then these biases will only be\nreinforced further.\nOn the other hand, diffusion models may be useful for data compression, which, as data becomes\nhigher resolution and as global internet trafﬁc increases, might be crucial to ensure accessibility of\nthe internet to wide audiences. Our work might contribute to representation learning on unlabeled\nraw data for a large", "page_start": 9, "page_end": 9, "char_count": 2500, "text_raw": "d\nimportance sampling [24]. Our progressive decoding argument can be seen in convolutional DRAW\nand related models [ 18, 40] and may also lead to more general designs for subscale orderings or\nsampling strategies for autoregressive models [38, 64].\n8\n\n[PAGE 9]\n6 Conclusion\nWe have presented high quality image samples using diffusion models, and we have found connections\namong diffusion models and variational inference for training Markov chains, denoising score\nmatching and annealed Langevin dynamics (and energy-based models by extension), autoregressive\nmodels, and progressive lossy compression. Since diffusion models seem to have excellent inductive\nbiases for image data, we look forward to investigating their utility in other data modalities and as\ncomponents in other types of generative models and machine learning systems.\nBroader Impact\nOur work on diffusion models takes on a similar scope as existing work on other types of deep\ngenerative models, such as efforts to improve the sample quality of GANs, ﬂows, autoregressive\nmodels, and so forth. Our paper represents progress in making diffusion models a generally useful\ntool in this family of techniques, so it may serve to amplify any impacts that generative models have\nhad (and will have) on the broader world.\nUnfortunately, there are numerous well-known malicious uses of generative models. Sample gen-\neration techniques can be employed to produce fake images and videos of high proﬁle ﬁgures for\npolitical purposes. While fake images were manually created long before software tools were avail-\nable, generative models such as ours make the process easier. Fortunately, CNN-generated images\ncurrently have subtle ﬂaws that allow detection [62], but improvements in generative models may\nmake this more difﬁcult. Generative models also reﬂect the biases in the datasets on which they\nare trained. As many large datasets are collected from the internet by automated systems, it can be\ndifﬁcult to remove these biases, especially when the images are unlabeled. If samples from generative\nmodels trained on these datasets proliferate throughout the internet, then these biases will only be\nreinforced further.\nOn the other hand, diffusion models may be useful for data compression, which, as data becomes\nhigher resolution and as global internet trafﬁc increases, might be crucial to ensure accessibility of\nthe internet to wide audiences. Our work might contribute to representation learning on unlabeled\nraw data for a large", "section": "conclusion", "text_embed": "d\nimportance sampling [24]. Our progressive decoding argument can be seen in convolutional DRAW\nand related models [ 18, 40] and may also lead to more general designs for subscale orderings or\nsampling strategies for autoregressive models [38, 64].\n8\n\n[PAGE 9]\n6 Conclusion\nWe have presented high quality image samples using diffusion models, and we have found connections\namong diffusion models and variational inference for training Markov chains, denoising score\nmatching and annealed Langevin dynamics (and energy-based models by extension), autoregressive\nmodels, and progressive lossy compression. Since diffusion models seem to have excellent inductive\nbiases for image data, we look forward to investigating their utility in other data modalities and as\ncomponents in other types of generative models and machine learning systems.\nBroader Impact\nOur work on diffusion models takes on a similar scope as existing work on other types of deep\ngenerative models, such as efforts to improve the sample quality of GANs, ﬂows, autoregressive\nmodels, and so forth. Our paper represents progress in making diffusion models a generally useful\ntool in this family of techniques, so it may serve to amplify any impacts that generative models have\nhad (and will have) on the broader world.\nUnfortunately, there are numerous well-known malicious uses of generative models. Sample gen-\neration techniques can be employed to produce fake images and videos of high proﬁle ﬁgures for\npolitical purposes. While fake images were manually created long before software tools were avail-\nable, generative models such as ours make the process easier. Fortunately, CNN-generated images\ncurrently have subtle ﬂaws that allow detection [62], but improvements in generative models may\nmake this more difﬁcult. Generative models also reﬂect the biases in the datasets on which they\nare trained. As many large datasets are collected from the internet by automated systems, it can be\ndifﬁcult to remove these biases, especially when the images are unlabeled. If samples from generative\nmodels trained on these datasets proliferate throughout the internet, then these biases will only be\nreinforced further.\nOn the other hand, diffusion models may be useful for data compression, which, as data becomes\nhigher resolution and as global internet trafﬁc increases, might be crucial to ensure accessibility of\nthe internet to wide audiences. Our work might contribute to representation learning on unlabeled\nraw data for a large"}
{"chunk_id": "YOUR_0024", "text": "rly on to make network size ﬁt within memory\nconstraints, we performed the majority of our hyperparameter search to optimize for CIFAR10 sample\nquality, then transferred the resulting settings over to the other datasets:\n• We chose the βt schedule from a set of constant, linear, and quadratic schedules, all\nconstrained so that LT ≈0. We set T = 1000 without a sweep, and we chose a linear\nschedule from β1 = 10−4 to βT = 0.02.\n• We set the dropout rate on CIFAR10 to0.1 by sweeping over the values {0.1,0.2,0.3,0.4}.\nWithout dropout on CIFAR10, we obtained poorer samples reminiscent of the overﬁtting\nartifacts in an unregularized PixelCNN++ [52]. We set dropout rate on the other datasets to\nzero without sweeping.\n• We used random horizontal ﬂips during training for CIFAR10; we tried training both with\nand without ﬂips, and found ﬂips to improve sample quality slightly. We also used random\nhorizontal ﬂips for all other datasets except LSUN Bedroom.\n• We tried Adam [31] and RMSProp early on in our experimentation process and chose the\nformer. We left the hyperparameters to their standard values. We set the learning rate to\n2 ×10−4 without any sweeping, and we lowered it to 2 ×10−5 for the 256 ×256 images,\nwhich seemed unstable to train with the larger learning rate.\n14\n\n[PAGE 15]\n• We set the batch size to 128 for CIFAR10 and 64 for larger images. We did not sweep over\nthese values.\n• We used EMA on model parameters with a decay factor of 0.9999. We did not sweep over\nthis value.\nFinal experiments were trained once and evaluated throughout training for sample quality. Sample\nquality scores and log likelihood are reported on the minimum FID value over the course of training.\nOn CIFAR10, we calculated Inception and FID scores on 50000 samples using the original code\nfrom the OpenAI [ 51] and TTUR [ 21] repositories, respectively. On LSUN, we calculated FID\nscores on 50000 samples using code from the StyleGAN2 [30] repository. CIFAR10 and CelebA-HQ\nwere loaded as provided by TensorFlow Datasets ( https://www.tensorflow.org/datasets),\nand LSUN was prepared using code from StyleGAN. Dataset splits (or lack thereof) are standard\nfrom the papers that introduced their usage in a generative modeling context. All details can be found\nin the source code release.\nC Discussion on related work\nOur model architecture, forward process deﬁnition, and prior differ from NCSN [55, 56] in subtle but\nimportant ways that improve sample quality, and, notably, we directly train our samp", "page_start": 15, "page_end": 15, "char_count": 2500, "text_raw": "rly on to make network size ﬁt within memory\nconstraints, we performed the majority of our hyperparameter search to optimize for CIFAR10 sample\nquality, then transferred the resulting settings over to the other datasets:\n• We chose the βt schedule from a set of constant, linear, and quadratic schedules, all\nconstrained so that LT ≈0. We set T = 1000 without a sweep, and we chose a linear\nschedule from β1 = 10−4 to βT = 0.02.\n• We set the dropout rate on CIFAR10 to0.1 by sweeping over the values {0.1,0.2,0.3,0.4}.\nWithout dropout on CIFAR10, we obtained poorer samples reminiscent of the overﬁtting\nartifacts in an unregularized PixelCNN++ [52]. We set dropout rate on the other datasets to\nzero without sweeping.\n• We used random horizontal ﬂips during training for CIFAR10; we tried training both with\nand without ﬂips, and found ﬂips to improve sample quality slightly. We also used random\nhorizontal ﬂips for all other datasets except LSUN Bedroom.\n• We tried Adam [31] and RMSProp early on in our experimentation process and chose the\nformer. We left the hyperparameters to their standard values. We set the learning rate to\n2 ×10−4 without any sweeping, and we lowered it to 2 ×10−5 for the 256 ×256 images,\nwhich seemed unstable to train with the larger learning rate.\n14\n\n[PAGE 15]\n• We set the batch size to 128 for CIFAR10 and 64 for larger images. We did not sweep over\nthese values.\n• We used EMA on model parameters with a decay factor of 0.9999. We did not sweep over\nthis value.\nFinal experiments were trained once and evaluated throughout training for sample quality. Sample\nquality scores and log likelihood are reported on the minimum FID value over the course of training.\nOn CIFAR10, we calculated Inception and FID scores on 50000 samples using the original code\nfrom the OpenAI [ 51] and TTUR [ 21] repositories, respectively. On LSUN, we calculated FID\nscores on 50000 samples using code from the StyleGAN2 [30] repository. CIFAR10 and CelebA-HQ\nwere loaded as provided by TensorFlow Datasets ( https://www.tensorflow.org/datasets),\nand LSUN was prepared using code from StyleGAN. Dataset splits (or lack thereof) are standard\nfrom the papers that introduced their usage in a generative modeling context. All details can be found\nin the source code release.\nC Discussion on related work\nOur model architecture, forward process deﬁnition, and prior differ from NCSN [55, 56] in subtle but\nimportant ways that improve sample quality, and, notably, we directly train our samp", "section": "unknown", "text_embed": "rly on to make network size ﬁt within memory\nconstraints, we performed the majority of our hyperparameter search to optimize for CIFAR10 sample\nquality, then transferred the resulting settings over to the other datasets:\n• We chose the βt schedule from a set of constant, linear, and quadratic schedules, all\nconstrained so that LT ≈0. We set T = 1000 without a sweep, and we chose a linear\nschedule from β1 = 10−4 to βT = 0.02.\n• We set the dropout rate on CIFAR10 to0.1 by sweeping over the values {0.1,0.2,0.3,0.4}.\nWithout dropout on CIFAR10, we obtained poorer samples reminiscent of the overﬁtting\nartifacts in an unregularized PixelCNN++ [52]. We set dropout rate on the other datasets to\nzero without sweeping.\n• We used random horizontal ﬂips during training for CIFAR10; we tried training both with\nand without ﬂips, and found ﬂips to improve sample quality slightly. We also used random\nhorizontal ﬂips for all other datasets except LSUN Bedroom.\n• We tried Adam [31] and RMSProp early on in our experimentation process and chose the\nformer. We left the hyperparameters to their standard values. We set the learning rate to\n2 ×10−4 without any sweeping, and we lowered it to 2 ×10−5 for the 256 ×256 images,\nwhich seemed unstable to train with the larger learning rate.\n14\n\n[PAGE 15]\n• We set the batch size to 128 for CIFAR10 and 64 for larger images. We did not sweep over\nthese values.\n• We used EMA on model parameters with a decay factor of 0.9999. We did not sweep over\nthis value.\nFinal experiments were trained once and evaluated throughout training for sample quality. Sample\nquality scores and log likelihood are reported on the minimum FID value over the course of training.\nOn CIFAR10, we calculated Inception and FID scores on 50000 samples using the original code\nfrom the OpenAI [ 51] and TTUR [ 21] repositories, respectively. On LSUN, we calculated FID\nscores on 50000 samples using code from the StyleGAN2 [30] repository. CIFAR10 and CelebA-HQ\nwere loaded as provided by TensorFlow Datasets ( https://www.tensorflow.org/datasets),\nand LSUN was prepared using code from StyleGAN. Dataset splits (or lack thereof) are standard\nfrom the papers that introduced their usage in a generative modeling context. All details can be found\nin the source code release.\nC Discussion on related work\nOur model architecture, forward process deﬁnition, and prior differ from NCSN [55, 56] in subtle but\nimportant ways that improve sample quality, and, notably, we directly train our samp"}
{"chunk_id": "YOUR_0025", "text": "e found\nin the source code release.\nC Discussion on related work\nOur model architecture, forward process deﬁnition, and prior differ from NCSN [55, 56] in subtle but\nimportant ways that improve sample quality, and, notably, we directly train our sampler as a latent\nvariable model rather than adding it after training post-hoc. In greater detail:\n1. We use a U-Net with self-attention; NCSN uses a ReﬁneNet with dilated convolutions. We\ncondition all layers on tby adding in the Transformer sinusoidal position embedding, rather\nthan only in normalization layers (NCSNv1) or only at the output (v2).\n2. Diffusion models scale down the data with each forward process step (by a √1 −βt factor)\nso that variance does not grow when adding noise, thus providing consistently scaled inputs\nto the neural net reverse process. NCSN omits this scaling factor.\n3. Unlike NCSN, our forward process destroys signal (DKL(q(xT|x0) ∥N(0,I)) ≈0), ensur-\ning a close match between the prior and aggregate posterior of xT. Also unlike NCSN, our\nβt are very small, which ensures that the forward process is reversible by a Markov chain\nwith conditional Gaussians. Both of these factors prevent distribution shift when sampling.\n4. Our Langevin-like sampler has coefﬁcients (learning rate, noise scale, etc.) derived rig-\norously from βt in the forward process. Thus, our training procedure directly trains our\nsampler to match the data distribution after T steps: it trains the sampler as a latent variable\nmodel using variational inference. In contrast, NCSN’s sampler coefﬁcients are set by hand\npost-hoc, and their training procedure is not guaranteed to directly optimize a quality metric\nof their sampler.\nD Samples\nAdditional samples Figure 11, 13, 16, 17, 18, and 19 show uncurated samples from the diffusion\nmodels trained on CelebA-HQ, CIFAR10 and LSUN datasets.\nLatent structure and reverse process stochasticity During sampling, both the prior xT ∼\nN(0,I) and Langevin dynamics are stochastic. To understand the signiﬁcance of the second source\nof noise, we sampled multiple images conditioned on the same intermediate latent for the CelebA\n256 ×256 dataset. Figure 7 shows multiple draws from the reverse process x0 ∼pθ(x0|xt) that\nshare the latent xt for t∈{1000,750,500,250}. To accomplish this, we run a single reverse chain\nfrom an initial draw from the prior. At the intermediate timesteps, the chain is split to sample multiple\nimages. When the chain is split after the prior draw at xT=1000, the samp", "page_start": 1, "page_end": 1, "char_count": 2500, "text_raw": "e found\nin the source code release.\nC Discussion on related work\nOur model architecture, forward process deﬁnition, and prior differ from NCSN [55, 56] in subtle but\nimportant ways that improve sample quality, and, notably, we directly train our sampler as a latent\nvariable model rather than adding it after training post-hoc. In greater detail:\n1. We use a U-Net with self-attention; NCSN uses a ReﬁneNet with dilated convolutions. We\ncondition all layers on tby adding in the Transformer sinusoidal position embedding, rather\nthan only in normalization layers (NCSNv1) or only at the output (v2).\n2. Diffusion models scale down the data with each forward process step (by a √1 −βt factor)\nso that variance does not grow when adding noise, thus providing consistently scaled inputs\nto the neural net reverse process. NCSN omits this scaling factor.\n3. Unlike NCSN, our forward process destroys signal (DKL(q(xT|x0) ∥N(0,I)) ≈0), ensur-\ning a close match between the prior and aggregate posterior of xT. Also unlike NCSN, our\nβt are very small, which ensures that the forward process is reversible by a Markov chain\nwith conditional Gaussians. Both of these factors prevent distribution shift when sampling.\n4. Our Langevin-like sampler has coefﬁcients (learning rate, noise scale, etc.) derived rig-\norously from βt in the forward process. Thus, our training procedure directly trains our\nsampler to match the data distribution after T steps: it trains the sampler as a latent variable\nmodel using variational inference. In contrast, NCSN’s sampler coefﬁcients are set by hand\npost-hoc, and their training procedure is not guaranteed to directly optimize a quality metric\nof their sampler.\nD Samples\nAdditional samples Figure 11, 13, 16, 17, 18, and 19 show uncurated samples from the diffusion\nmodels trained on CelebA-HQ, CIFAR10 and LSUN datasets.\nLatent structure and reverse process stochasticity During sampling, both the prior xT ∼\nN(0,I) and Langevin dynamics are stochastic. To understand the signiﬁcance of the second source\nof noise, we sampled multiple images conditioned on the same intermediate latent for the CelebA\n256 ×256 dataset. Figure 7 shows multiple draws from the reverse process x0 ∼pθ(x0|xt) that\nshare the latent xt for t∈{1000,750,500,250}. To accomplish this, we run a single reverse chain\nfrom an initial draw from the prior. At the intermediate timesteps, the chain is split to sample multiple\nimages. When the chain is split after the prior draw at xT=1000, the samp", "section": "unknown", "text_embed": "e found\nin the source code release.\nC Discussion on related work\nOur model architecture, forward process deﬁnition, and prior differ from NCSN [55, 56] in subtle but\nimportant ways that improve sample quality, and, notably, we directly train our sampler as a latent\nvariable model rather than adding it after training post-hoc. In greater detail:\n1. We use a U-Net with self-attention; NCSN uses a ReﬁneNet with dilated convolutions. We\ncondition all layers on tby adding in the Transformer sinusoidal position embedding, rather\nthan only in normalization layers (NCSNv1) or only at the output (v2).\n2. Diffusion models scale down the data with each forward process step (by a √1 −βt factor)\nso that variance does not grow when adding noise, thus providing consistently scaled inputs\nto the neural net reverse process. NCSN omits this scaling factor.\n3. Unlike NCSN, our forward process destroys signal (DKL(q(xT|x0) ∥N(0,I)) ≈0), ensur-\ning a close match between the prior and aggregate posterior of xT. Also unlike NCSN, our\nβt are very small, which ensures that the forward process is reversible by a Markov chain\nwith conditional Gaussians. Both of these factors prevent distribution shift when sampling.\n4. Our Langevin-like sampler has coefﬁcients (learning rate, noise scale, etc.) derived rig-\norously from βt in the forward process. Thus, our training procedure directly trains our\nsampler to match the data distribution after T steps: it trains the sampler as a latent variable\nmodel using variational inference. In contrast, NCSN’s sampler coefﬁcients are set by hand\npost-hoc, and their training procedure is not guaranteed to directly optimize a quality metric\nof their sampler.\nD Samples\nAdditional samples Figure 11, 13, 16, 17, 18, and 19 show uncurated samples from the diffusion\nmodels trained on CelebA-HQ, CIFAR10 and LSUN datasets.\nLatent structure and reverse process stochasticity During sampling, both the prior xT ∼\nN(0,I) and Langevin dynamics are stochastic. To understand the signiﬁcance of the second source\nof noise, we sampled multiple images conditioned on the same intermediate latent for the CelebA\n256 ×256 dataset. Figure 7 shows multiple draws from the reverse process x0 ∼pθ(x0|xt) that\nshare the latent xt for t∈{1000,750,500,250}. To accomplish this, we run a single reverse chain\nfrom an initial draw from the prior. At the intermediate timesteps, the chain is split to sample multiple\nimages. When the chain is split after the prior draw at xT=1000, the samp"}
{"chunk_id": "YOUR_0026", "text": "∈{1000,750,500,250}. To accomplish this, we run a single reverse chain\nfrom an initial draw from the prior. At the intermediate timesteps, the chain is split to sample multiple\nimages. When the chain is split after the prior draw at xT=1000, the samples differ signiﬁcantly.\nHowever, when the chain is split after more steps, samples share high-level attributes like gender,\nhair color, eyewear, saturation, pose and facial expression. This indicates that intermediate latents\nlike x750 encode these attributes, despite their imperceptibility.\nCoarse-to-ﬁne interpolation Figure 9 shows interpolations between a pair of source CelebA\n256 ×256 images as we vary the number of diffusion steps prior to latent space interpolation.\nIncreasing the number of diffusion steps destroys more structure in the source images, which the\n15\n\n[PAGE 16]\nmodel completes during the reverse process. This allows us to interpolate at both ﬁne granularities\nand coarse granularities. In the limiting case of 0 diffusion steps, the interpolation mixes source\nimages in pixel space. On the other hand, after 1000 diffusion steps, source information is lost and\ninterpolations are novel samples.\nSource Rec. λ=0.1 λ=0.2 λ=0.3 λ=0.4 λ=0.5 λ=0.6 λ=0.7 λ=0.8 λ=0.9 Rec. Source\n1000 steps\n875 steps\n750 steps\n625 steps\n500 steps\n375 steps\n250 steps\n125 steps\n0 steps\nFigure 9: Coarse-to-ﬁne interpolations that vary the number of diffusion steps prior to latent mixing.\n0 200 400 600 800 1,000\n2\n4\n6\n8\n10\nReverse process steps (T −t)\nInception Score\n0 200 400 600 800 1,000\n0\n100\n200\n300\nReverse process steps (T −t)\nFID\nFigure 10: Unconditional CIFAR10 progressive sampling quality over time\n16\n\n[PAGE 17]\nFigure 11: CelebA-HQ 256 ×256 generated samples\n17\n\n[PAGE 18]\n(a) Pixel space nearest neighbors\n(b) Inception feature space nearest neighbors\nFigure 12: CelebA-HQ 256 ×256 nearest neighbors, computed on a 100 ×100 crop surrounding the\nfaces. Generated samples are in the leftmost column, and training set nearest neighbors are in the\nremaining columns.\n18\n\n[PAGE 19]\nFigure 13: Unconditional CIFAR10 generated samples\n19\n\n[PAGE 20]\nFigure 14: Unconditional CIFAR10 progressive generation\n20\n\n[PAGE 21]\n(a) Pixel space nearest neighbors\n(b) Inception feature space nearest neighbors\nFigure 15: Unconditional CIFAR10 nearest neighbors. Generated samples are in the leftmost column,\nand training set nearest neighbors are in the remaining columns.\n21\n\n[PAGE 22]\nFigure 16: LSUN Church generated samples. FID=7.89\n22", "page_start": 16, "page_end": 22, "char_count": 2493, "text_raw": "∈{1000,750,500,250}. To accomplish this, we run a single reverse chain\nfrom an initial draw from the prior. At the intermediate timesteps, the chain is split to sample multiple\nimages. When the chain is split after the prior draw at xT=1000, the samples differ signiﬁcantly.\nHowever, when the chain is split after more steps, samples share high-level attributes like gender,\nhair color, eyewear, saturation, pose and facial expression. This indicates that intermediate latents\nlike x750 encode these attributes, despite their imperceptibility.\nCoarse-to-ﬁne interpolation Figure 9 shows interpolations between a pair of source CelebA\n256 ×256 images as we vary the number of diffusion steps prior to latent space interpolation.\nIncreasing the number of diffusion steps destroys more structure in the source images, which the\n15\n\n[PAGE 16]\nmodel completes during the reverse process. This allows us to interpolate at both ﬁne granularities\nand coarse granularities. In the limiting case of 0 diffusion steps, the interpolation mixes source\nimages in pixel space. On the other hand, after 1000 diffusion steps, source information is lost and\ninterpolations are novel samples.\nSource Rec. λ=0.1 λ=0.2 λ=0.3 λ=0.4 λ=0.5 λ=0.6 λ=0.7 λ=0.8 λ=0.9 Rec. Source\n1000 steps\n875 steps\n750 steps\n625 steps\n500 steps\n375 steps\n250 steps\n125 steps\n0 steps\nFigure 9: Coarse-to-ﬁne interpolations that vary the number of diffusion steps prior to latent mixing.\n0 200 400 600 800 1,000\n2\n4\n6\n8\n10\nReverse process steps (T −t)\nInception Score\n0 200 400 600 800 1,000\n0\n100\n200\n300\nReverse process steps (T −t)\nFID\nFigure 10: Unconditional CIFAR10 progressive sampling quality over time\n16\n\n[PAGE 17]\nFigure 11: CelebA-HQ 256 ×256 generated samples\n17\n\n[PAGE 18]\n(a) Pixel space nearest neighbors\n(b) Inception feature space nearest neighbors\nFigure 12: CelebA-HQ 256 ×256 nearest neighbors, computed on a 100 ×100 crop surrounding the\nfaces. Generated samples are in the leftmost column, and training set nearest neighbors are in the\nremaining columns.\n18\n\n[PAGE 19]\nFigure 13: Unconditional CIFAR10 generated samples\n19\n\n[PAGE 20]\nFigure 14: Unconditional CIFAR10 progressive generation\n20\n\n[PAGE 21]\n(a) Pixel space nearest neighbors\n(b) Inception feature space nearest neighbors\nFigure 15: Unconditional CIFAR10 nearest neighbors. Generated samples are in the leftmost column,\nand training set nearest neighbors are in the remaining columns.\n21\n\n[PAGE 22]\nFigure 16: LSUN Church generated samples. FID=7.89\n22", "section": "unknown", "text_embed": "∈{1000,750,500,250}. To accomplish this, we run a single reverse chain\nfrom an initial draw from the prior. At the intermediate timesteps, the chain is split to sample multiple\nimages. When the chain is split after the prior draw at xT=1000, the samples differ signiﬁcantly.\nHowever, when the chain is split after more steps, samples share high-level attributes like gender,\nhair color, eyewear, saturation, pose and facial expression. This indicates that intermediate latents\nlike x750 encode these attributes, despite their imperceptibility.\nCoarse-to-ﬁne interpolation Figure 9 shows interpolations between a pair of source CelebA\n256 ×256 images as we vary the number of diffusion steps prior to latent space interpolation.\nIncreasing the number of diffusion steps destroys more structure in the source images, which the\n15\n\n[PAGE 16]\nmodel completes during the reverse process. This allows us to interpolate at both ﬁne granularities\nand coarse granularities. In the limiting case of 0 diffusion steps, the interpolation mixes source\nimages in pixel space. On the other hand, after 1000 diffusion steps, source information is lost and\ninterpolations are novel samples.\nSource Rec. λ=0.1 λ=0.2 λ=0.3 λ=0.4 λ=0.5 λ=0.6 λ=0.7 λ=0.8 λ=0.9 Rec. Source\n1000 steps\n875 steps\n750 steps\n625 steps\n500 steps\n375 steps\n250 steps\n125 steps\n0 steps\nFigure 9: Coarse-to-ﬁne interpolations that vary the number of diffusion steps prior to latent mixing.\n0 200 400 600 800 1,000\n2\n4\n6\n8\n10\nReverse process steps (T −t)\nInception Score\n0 200 400 600 800 1,000\n0\n100\n200\n300\nReverse process steps (T −t)\nFID\nFigure 10: Unconditional CIFAR10 progressive sampling quality over time\n16\n\n[PAGE 17]\nFigure 11: CelebA-HQ 256 ×256 generated samples\n17\n\n[PAGE 18]\n(a) Pixel space nearest neighbors\n(b) Inception feature space nearest neighbors\nFigure 12: CelebA-HQ 256 ×256 nearest neighbors, computed on a 100 ×100 crop surrounding the\nfaces. Generated samples are in the leftmost column, and training set nearest neighbors are in the\nremaining columns.\n18\n\n[PAGE 19]\nFigure 13: Unconditional CIFAR10 generated samples\n19\n\n[PAGE 20]\nFigure 14: Unconditional CIFAR10 progressive generation\n20\n\n[PAGE 21]\n(a) Pixel space nearest neighbors\n(b) Inception feature space nearest neighbors\nFigure 15: Unconditional CIFAR10 nearest neighbors. Generated samples are in the leftmost column,\nand training set nearest neighbors are in the remaining columns.\n21\n\n[PAGE 22]\nFigure 16: LSUN Church generated samples. FID=7.89\n22"}
{"chunk_id": "YOUR_0027", "text": "e nearest neighbors\nFigure 15: Unconditional CIFAR10 nearest neighbors. Generated samples are in the leftmost column,\nand training set nearest neighbors are in the remaining columns.\n21\n\n[PAGE 22]\nFigure 16: LSUN Church generated samples. FID=7.89\n22\n\n[PAGE 23]\nFigure 17: LSUN Bedroom generated samples, large model. FID=4.90\n23\n\n[PAGE 24]\nFigure 18: LSUN Bedroom generated samples, small model. FID=6.36\n24\n\n[PAGE 25]\nFigure 19: LSUN Cat generated samples. FID=19.75\n25", "page_start": 22, "page_end": 25, "char_count": 471, "text_raw": "e nearest neighbors\nFigure 15: Unconditional CIFAR10 nearest neighbors. Generated samples are in the leftmost column,\nand training set nearest neighbors are in the remaining columns.\n21\n\n[PAGE 22]\nFigure 16: LSUN Church generated samples. FID=7.89\n22\n\n[PAGE 23]\nFigure 17: LSUN Bedroom generated samples, large model. FID=4.90\n23\n\n[PAGE 24]\nFigure 18: LSUN Bedroom generated samples, small model. FID=6.36\n24\n\n[PAGE 25]\nFigure 19: LSUN Cat generated samples. FID=19.75\n25", "section": "unknown", "text_embed": "e nearest neighbors\nFigure 15: Unconditional CIFAR10 nearest neighbors. Generated samples are in the leftmost column,\nand training set nearest neighbors are in the remaining columns.\n21\n\n[PAGE 22]\nFigure 16: LSUN Church generated samples. FID=7.89\n22\n\n[PAGE 23]\nFigure 17: LSUN Bedroom generated samples, large model. FID=4.90\n23\n\n[PAGE 24]\nFigure 18: LSUN Bedroom generated samples, small model. FID=6.36\n24\n\n[PAGE 25]\nFigure 19: LSUN Cat generated samples. FID=19.75\n25"}
